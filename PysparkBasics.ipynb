{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![図1.2 Sparkの紹介とインストール](images/sparkBasics.png)\n",
    "\n",
    "# 本セクションの目次\n",
    "1. Spark Sessionとは？\n",
    "2. Sparkを用いたバッチにおけるエンジニアリングの流れ(データソース、変換、カラムナーフォーマット、テーブル作成)？\n",
    "3. データソースの読み込み\n",
    "4. データ変換\n",
    "5. DIKWモデル\n",
    "6. カラムナーフォーマットへの変換\n",
    "7. スモールファイル問題\n",
    "8. 参照用のテーブル作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Sessionとは？\n",
    "\n",
    "Javaで言うところのインスタンスを作る作業のことです(new Class())。  \n",
    "今回の場合は、アプリケーション名が「chapter2」で作成を行っています。  \n",
    "\n",
    "```\n",
    "#pysparkに必要なライブラリを読み込む\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#spark sessionの作成\n",
    "# spark.ui.enabled trueとするとSparkのGUI画面を確認することができます\n",
    "# spark.eventLog.enabled true　とすると　GUIで実行ログを確認することができます\n",
    "# GUIなどの確認は次のチャプターで説明を行います。\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"chapter2\") \\\n",
    "    .config(\"hive.exec.dynamic.partition\", \"true\") \\\n",
    "    .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"JST\") \\\n",
    "    .config(\"spark.ui.enabled\",\"true\") \\\n",
    "    .config(\"spark.eventLog.enabled\",\"true\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "```\n",
    "\n",
    "configの部分で  \n",
    "非常に細かい設定ができるので、詳しくは公式のドキュメントを参考にしてください https://spark.apache.org/docs/3.1.1/  \n",
    "一部メモリの設定いついては「Sparkを本番環境で動かす」チャプターにて紹介します"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkを用いたバッチにおけるデータエンジニアリング一連の流れ\n",
    "\n",
    "1. データソースの読み込み(今回は、人口統計データ(/dataset/jinko.csv))　ETL(Extract Transform Load)で言うEの部分\n",
    "2. 変換を行う(集計等を行う)　DataFrame処理 or SQL処理の２パターンで実行可能 ETL(Extract Transform Load)で言うTの部分\n",
    "3. カラムナーフォーマットへ変換する ETL(Extract Transform Load)で言うTの部分\n",
    "4. 出力したデータをみんなに見やすくするため(BIツールから参照できるように)テーブルを作成する ETL(Extract Transform Load)で言うLの部分\n",
    "\n",
    "よくある、関数の羅列をするのではなく、実業務に沿った形で流れを紹介していきます。\n",
    "\n",
    "## データソース\n",
    "データの源。リレーショナルデータベースのときもあれば、今回のようにファイルの形式のときもある。  \n",
    "更に進むと、PDFやEXCELなんて事もあります。  \n",
    "ストリーミングだとIoTであったり、Webブラウザのアクセスログだったりとデータになりうるものは無限に存在しています。  \n",
    "\n",
    "## 変換処理\n",
    "ETL（Extract Transform Load）というと少し定義として広いのかもしれないのですが、  \n",
    "データを整形してより分析向けの形(フォーマット変換や圧縮含む)にしたり、精度の高いデータを作成する行為のことです。  \n",
    "そのため、ETLというとバッチ処理のイメージを持つ人も多いかもしれませんが、ストリーミングデータにも適用される言葉です。   \n",
    "Tの処理はSparkにおいてDataFrameもしくはSQLで処理することができる(RDDと呼ばれるものもあるが、労力の割に実際は出番はあまりなく今回は取り扱わない)\n",
    "\n",
    "## カラムナーフォーマットへ変換を行う\n",
    "ビッグデータの世界では、Apache Parquet と呼ばれるフォーマットが広く使われています。  \n",
    "CSV形式のようなローフォーマットはビックデータ処理において処理効率が悪いため、早い段階でParquetに変換を行います。  \n",
    "分析用のSQLの実行であったり、複数台で処理することに向いているフォーマットです。  \n",
    "\n",
    "Parquetの特徴としては以下になります。\n",
    "\n",
    "- カラムナー（ストレージ）フォーマット\n",
    "- カラムごとに圧縮が効くため、効率よくデータをストアできる\n",
    "- 多くのプロダクトがサポートしている\n",
    "\n",
    "多くのプロダクトはParquetを取り込んだり処理したりする機能を提供してくれており単体ではなく総合で使えるフォーマットです。\n",
    "\n",
    "## テーブル形式での保存\n",
    "\n",
    "多くは、実データとテーブル定義が分離された`ロケーション方式`をとっている。  \n",
    "後ほど実際に作成してみますが、イメージは以下のような感じです。\n",
    "\n",
    "```\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS sample.sampletable ( id INT, date STRING)\n",
    "PARTITIONED BY (dt INT)\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "LOCATION '/Users/yuki/pyspark_batch/dataset/parquet/';\n",
    "\n",
    "#S3などであれば、以下のように設定を変えることも可能です。\n",
    "LOCATION 's3://data.platform/sample.db/raw_zone/sampletable/';\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "テーブルとして保存することによって、非エンジニアにも扱いやすくしてデータを提供することが可能です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: hive.exec.dynamic.partition.mode\n",
      "Warning: Ignoring non-Spark config property: hive.exec.dynamic.partition\n",
      "21/10/11 13:17:04 WARN Utils: Your hostname, saitouyuuki-no-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.0.10 instead (on interface en0)\n",
      "21/10/11 13:17:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/yuki/Library/Python/3.8/lib/python/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "21/10/11 13:17:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "#pysparkに必要なライブラリを読み込む\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#spark sessionの作成\n",
    "# spark.ui.enabled trueとするとSparkのGUI画面を確認することができます\n",
    "# spark.eventLog.enabled true　とすると　GUIで実行ログを確認することができます\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"chapter2\") \\\n",
    "    .config(\"hive.exec.dynamic.partition\", \"true\") \\\n",
    "    .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"JST\") \\\n",
    "    .config(\"spark.ui.enabled\",\"true\") \\\n",
    "    .config(\"spark.eventLog.enabled\",\"true\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データソースの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "982"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#データソースの読み込み\n",
    "#sep='\\t'とすればtsvでも読み込みが可能です\n",
    "#multiLineは、CSVやTSVの各カラムに改行が含まれていた時の対策です。\n",
    "df=spark.read.option(\"multiLine\", \"true\").option(\"encoding\", \"SJIS\").csv(\"/Users/yuki/pyspark_batch/dataset/jinko.csv\", header=True, sep=',', inferSchema=False)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+----+----------+----------+----+------------+----------+----------+\n",
      "|都道府県コード|都道府県名|元号|和暦（年）|西暦（年）|注  |人口（総数）|人口（男）|人口（女）|\n",
      "+--------------+----------+----+----------+----------+----+------------+----------+----------+\n",
      "|00            |全国      |大正|9         |1920      |null|55963053    |28044185  |27918868  |\n",
      "|01            |北海道    |大正|9         |1920      |null|2359183     |1244322   |1114861   |\n",
      "|02            |青森県    |大正|9         |1920      |null|756454      |381293    |375161    |\n",
      "|03            |岩手県    |大正|9         |1920      |null|845540      |421069    |424471    |\n",
      "|04            |宮城県    |大正|9         |1920      |null|961768      |485309    |476459    |\n",
      "|05            |秋田県    |大正|9         |1920      |null|898537      |453682    |444855    |\n",
      "|06            |山形県    |大正|9         |1920      |null|968925      |478328    |490597    |\n",
      "|07            |福島県    |大正|9         |1920      |null|1362750     |673525    |689225    |\n",
      "|08            |茨城県    |大正|9         |1920      |null|1350400     |662128    |688272    |\n",
      "|09            |栃木県    |大正|9         |1920      |null|1046479     |514255    |532224    |\n",
      "|10            |群馬県    |大正|9         |1920      |null|1052610     |514106    |538504    |\n",
      "|11            |埼玉県    |大正|9         |1920      |null|1319533     |641161    |678372    |\n",
      "|12            |千葉県    |大正|9         |1920      |null|1336155     |656968    |679187    |\n",
      "|13            |東京都    |大正|9         |1920      |null|3699428     |1952989   |1746439   |\n",
      "|14            |神奈川県  |大正|9         |1920      |null|1323390     |689751    |633639    |\n",
      "|15            |新潟県    |大正|9         |1920      |null|1776474     |871532    |904942    |\n",
      "|16            |富山県    |大正|9         |1920      |null|724276      |354775    |369501    |\n",
      "|17            |石川県    |大正|9         |1920      |null|747360      |364375    |382985    |\n",
      "|18            |福井県    |大正|9         |1920      |null|599155      |293181    |305974    |\n",
      "|19            |山梨県    |大正|9         |1920      |null|583453      |290817    |292636    |\n",
      "+--------------+----------+----+----------+----------+----+------------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)\n",
    "#truncate Falseで表示結果を省略せず表示します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 変換を行う(集計等を行う)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1624560064.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/63/4b1jyj4n5g973562db46kc5h0000gn/T/ipykernel_90561/1624560064.py\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    df_after_t=df.where(df.\"和暦（年）\"== \"平成\")\u001b[0m\n\u001b[0m                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# ここからはETLにおけるTを行っていきます\n",
    "\n",
    "#大正や昭和はもう不要かなと感じたら変換処理にて\n",
    "df_after_t=df.where(df.\"和暦（年）\"== \"平成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+-----+----------+----------+----+------------+----------+------------+\n",
      "|          code|    kenmei|gengo|    wareki|   seireki| chu|       sokei|jinko_male|jinko_female|\n",
      "+--------------+----------+-----+----------+----------+----+------------+----------+------------+\n",
      "|都道府県コード|都道府県名| 元号|和暦（年）|西暦（年）|  注|人口（総数）|人口（男）|  人口（女）|\n",
      "|            00|      全国| 大正|         9|      1920|null|    55963053|  28044185|    27918868|\n",
      "|            01|    北海道| 大正|         9|      1920|null|     2359183|   1244322|     1114861|\n",
      "|            02|    青森県| 大正|         9|      1920|null|      756454|    381293|      375161|\n",
      "|            03|    岩手県| 大正|         9|      1920|null|      845540|    421069|      424471|\n",
      "|            04|    宮城県| 大正|         9|      1920|null|      961768|    485309|      476459|\n",
      "|            05|    秋田県| 大正|         9|      1920|null|      898537|    453682|      444855|\n",
      "|            06|    山形県| 大正|         9|      1920|null|      968925|    478328|      490597|\n",
      "|            07|    福島県| 大正|         9|      1920|null|     1362750|    673525|      689225|\n",
      "|            08|    茨城県| 大正|         9|      1920|null|     1350400|    662128|      688272|\n",
      "|            09|    栃木県| 大正|         9|      1920|null|     1046479|    514255|      532224|\n",
      "|            10|    群馬県| 大正|         9|      1920|null|     1052610|    514106|      538504|\n",
      "|            11|    埼玉県| 大正|         9|      1920|null|     1319533|    641161|      678372|\n",
      "|            12|    千葉県| 大正|         9|      1920|null|     1336155|    656968|      679187|\n",
      "|            13|    東京都| 大正|         9|      1920|null|     3699428|   1952989|     1746439|\n",
      "|            14|  神奈川県| 大正|         9|      1920|null|     1323390|    689751|      633639|\n",
      "|            15|    新潟県| 大正|         9|      1920|null|     1776474|    871532|      904942|\n",
      "|            16|    富山県| 大正|         9|      1920|null|      724276|    354775|      369501|\n",
      "|            17|    石川県| 大正|         9|      1920|null|      747360|    364375|      382985|\n",
      "|            18|    福井県| 大正|         9|      1920|null|      599155|    293181|      305974|\n",
      "+--------------+----------+-----+----------+----------+----+------------+----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#うーん使いづらい。。(日本語))\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "#スキーマ設定をしていきましょう\n",
    "# カラム名、型、NullOKか？で設定していきます\n",
    "struct = StructType([\n",
    "    StructField(\"code\", StringType(), False),\n",
    "    StructField(\"kenmei\", StringType(), False),\n",
    "    StructField(\"gengo\", StringType(), False),\n",
    "    StructField(\"wareki\", StringType(), False),\n",
    "    StructField(\"seireki\", StringType(), False),\n",
    "    StructField(\"chu\", StringType(), False),\n",
    "    StructField(\"sokei\", StringType(), False),\n",
    "    StructField(\"jinko_male\", StringType(), False),\n",
    "    StructField(\"jinko_female\", StringType(), False)\n",
    "])\n",
    "df=spark.read.option(\"multiLine\", \"true\").option(\"encoding\", \"SJIS\") \\\n",
    "    .csv(\"/Users/yuki/pyspark_batch/dataset/jinko.csv\", header=False, sep=',', inferSchema=False,schema=struct)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------------+-----+------+-------+----+---------+----------+------------+\n",
      "|code|                kenmei|gengo|wareki|seireki| chu|    sokei|jinko_male|jinko_female|\n",
      "+----+----------------------+-----+------+-------+----+---------+----------+------------+\n",
      "|  00|                  全国| 平成|     2|   1990|null|123611167|  60696724|    62914443|\n",
      "|  0A|          人口集中地区| 平成|     2|   1990|null| 78152452|  38564229|    39588223|\n",
      "|  0B|人口集中地区以外の地区| 平成|     2|   1990|null| 45458715|  22132495|    23326220|\n",
      "|  01|                北海道| 平成|     2|   1990|null|  5643647|   2722988|     2920659|\n",
      "|  02|                青森県| 平成|     2|   1990|null|  1482873|    704758|      778115|\n",
      "|  03|                岩手県| 平成|     2|   1990|null|  1416928|    680197|      736731|\n",
      "|  04|                宮城県| 平成|     2|   1990|null|  2248558|   1105103|     1143455|\n",
      "|  05|                秋田県| 平成|     2|   1990|null|  1227478|    584678|      642800|\n",
      "|  06|                山形県| 平成|     2|   1990|null|  1258390|    607041|      651349|\n",
      "|  07|                福島県| 平成|     2|   1990|null|  2104058|   1024354|     1079704|\n",
      "|  08|                茨城県| 平成|     2|   1990|null|  2845382|   1419117|     1426265|\n",
      "|  09|                栃木県| 平成|     2|   1990|null|  1935168|    962571|      972597|\n",
      "|  10|                群馬県| 平成|     2|   1990|null|  1966265|    971704|      994561|\n",
      "|  11|                埼玉県| 平成|     2|   1990|null|  6405319|   3245868|     3159451|\n",
      "|  12|                千葉県| 平成|     2|   1990|null|  5555429|   2802774|     2752655|\n",
      "|  13|                東京都| 平成|     2|   1990|null| 11855563|   5969773|     5885790|\n",
      "|  14|              神奈川県| 平成|     2|   1990|null|  7980391|   4098147|     3882244|\n",
      "|  15|                新潟県| 平成|     2|   1990|null|  2474583|   1200376|     1274207|\n",
      "|  16|                富山県| 平成|     2|   1990|null|  1120161|    538640|      581521|\n",
      "|  17|                石川県| 平成|     2|   1990|null|  1164628|    562684|      601944|\n",
      "+----+----------------------+-----+------+-------+----+---------+----------+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----+----------------------+-----+------+-------+----+---------+----------+------------+\n",
      "|code|                kenmei|gengo|wareki|seireki| chu|    sokei|jinko_male|jinko_female|\n",
      "+----+----------------------+-----+------+-------+----+---------+----------+------------+\n",
      "|  00|                  全国| 平成|     2|   1990|null|123611167|  60696724|    62914443|\n",
      "|  0A|          人口集中地区| 平成|     2|   1990|null| 78152452|  38564229|    39588223|\n",
      "|  0B|人口集中地区以外の地区| 平成|     2|   1990|null| 45458715|  22132495|    23326220|\n",
      "|  01|                北海道| 平成|     2|   1990|null|  5643647|   2722988|     2920659|\n",
      "|  02|                青森県| 平成|     2|   1990|null|  1482873|    704758|      778115|\n",
      "|  03|                岩手県| 平成|     2|   1990|null|  1416928|    680197|      736731|\n",
      "|  04|                宮城県| 平成|     2|   1990|null|  2248558|   1105103|     1143455|\n",
      "|  05|                秋田県| 平成|     2|   1990|null|  1227478|    584678|      642800|\n",
      "|  06|                山形県| 平成|     2|   1990|null|  1258390|    607041|      651349|\n",
      "|  07|                福島県| 平成|     2|   1990|null|  2104058|   1024354|     1079704|\n",
      "|  08|                茨城県| 平成|     2|   1990|null|  2845382|   1419117|     1426265|\n",
      "|  09|                栃木県| 平成|     2|   1990|null|  1935168|    962571|      972597|\n",
      "|  10|                群馬県| 平成|     2|   1990|null|  1966265|    971704|      994561|\n",
      "|  11|                埼玉県| 平成|     2|   1990|null|  6405319|   3245868|     3159451|\n",
      "|  12|                千葉県| 平成|     2|   1990|null|  5555429|   2802774|     2752655|\n",
      "|  13|                東京都| 平成|     2|   1990|null| 11855563|   5969773|     5885790|\n",
      "|  14|              神奈川県| 平成|     2|   1990|null|  7980391|   4098147|     3882244|\n",
      "|  15|                新潟県| 平成|     2|   1990|null|  2474583|   1200376|     1274207|\n",
      "|  16|                富山県| 平成|     2|   1990|null|  1120161|    538640|      581521|\n",
      "|  17|                石川県| 平成|     2|   1990|null|  1164628|    562684|      601944|\n",
      "+----+----------------------+-----+------+-------+----+---------+----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 大正や昭はもう不要かなと感じたら変換処理にて\n",
    "df.where(df.gengo == \"平成\").show()\n",
    "# where 以外にもfilterと呼ばれるものがあります。機能は同じなので好きな方を選んで大丈夫です\n",
    "df.filter(df.gengo == \"平成\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+--------------------+--------------------+\n",
      "|                kenmei|            male_avg|          female_avg|\n",
      "+----------------------+--------------------+--------------------+\n",
      "|人口集中地区以外の地区|2.0976203166666668E7|2.2272045166666668E7|\n",
      "|                佐賀県|            408192.5|            456442.5|\n",
      "|                栃木県|   987741.8333333334|            999415.5|\n",
      "|                京都府|  1268325.3333333333|  1360099.3333333333|\n",
      "|                香川県|   485871.8333333333|   523763.6666666667|\n",
      "|                愛媛県|   692188.3333333334|   774376.1666666666|\n",
      "|                秋田県|   542928.3333333334|            604578.5|\n",
      "|                広島県|           1387308.5|  1478006.8333333333|\n",
      "|                宮崎県|            542386.5|            608793.0|\n",
      "|              鹿児島県|            818506.0|            929134.0|\n",
      "|                埼玉県|  3492880.3333333335|  3443447.8333333335|\n",
      "|                三重県|            893167.5|   944959.6666666666|\n",
      "|                島根県|            356034.5|   388621.6666666667|\n",
      "|                徳島県|   383399.1666666667|            423152.0|\n",
      "|                岐阜県|  1009389.1666666666|  1073025.1666666667|\n",
      "|                新潟県|           1175463.5|  1249345.3333333333|\n",
      "|                山形県|            583603.5|   627811.6666666666|\n",
      "|              神奈川県|           4360756.0|           4252978.5|\n",
      "|                群馬県|   986385.1666666666|  1013610.6666666666|\n",
      "|                岩手県|   659592.6666666666|   714973.1666666666|\n",
      "+----------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#集計をしてみます\n",
    "#平成の県ごとの男女の数の平均\n",
    "#groupByは県名ごとにグルーピングする記述です。\n",
    "#aggはカラムごとに集計する関数で今回は男性の人口と、女性の人口毎の平均を集計しています。\n",
    "#aliasは別名をつける関数です。例えば、女性の人口毎の平均の結果にはfemale_avgという別名を付けています。\n",
    "import pyspark.sql.functions as sf\n",
    "df.where(df.gengo == \"平成\").groupBy(\"kenmei\") \\\n",
    "  .agg(sf.avg(\"jinko_male\").alias(\"male_avg\"),sf.avg(\"jinko_female\").alias(\"female_avg\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# このような集計をクロス集計と呼びます\n",
    "クロス集計とは複数の項目をかけ合わせて集計を行うことです。\n",
    "今回の場合は、県名と男女をかけ合わせてそれぞれの県別、男女別のクロス集計を行いました。\n",
    "\n",
    "逆に、県名だけの集計などは単純集計と呼ばれています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データクレンジング\n",
    "データを操作していると、時には不要と感じる事もあります。 \n",
    "不要でデータを除外したり、並び替えたりより精度の高いデータにすることをデータクレンジングと呼びます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+-----------------+\n",
      "|  kenmei|         male_avg|       female_avg|\n",
      "+--------+-----------------+-----------------+\n",
      "|  鳥取県|287885.3333333333|314291.3333333333|\n",
      "|  島根県|         356034.5|388621.6666666667|\n",
      "|  高知県|372268.1666666667|         418517.0|\n",
      "|  徳島県|383399.1666666667|         423152.0|\n",
      "|  福井県|         395512.5|420182.6666666667|\n",
      "|  佐賀県|         408192.5|         456442.5|\n",
      "|  山梨県|425777.8333333333|441831.1666666667|\n",
      "|  香川県|485871.8333333333|523763.6666666667|\n",
      "|和歌山県|         490624.0|547112.3333333334|\n",
      "|  富山県|         532857.0|573049.8333333334|\n",
      "|  宮崎県|         542386.5|         608793.0|\n",
      "|  秋田県|542928.3333333334|         604578.5|\n",
      "|  石川県|         566064.0|         604518.5|\n",
      "|  大分県|571530.6666666666|638773.6666666666|\n",
      "|  山形県|         583603.5|627811.6666666666|\n",
      "|  沖縄県|         654622.0|679050.6666666666|\n",
      "|  岩手県|659592.6666666666|714973.1666666666|\n",
      "|  滋賀県|         662391.0|         680326.0|\n",
      "|  奈良県|671178.6666666666|734736.6666666666|\n",
      "|  青森県|675238.6666666666|751182.1666666666|\n",
      "+--------+-----------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#「人口集中地区以外の地区」がいらなそうですね。\n",
    "# データをクレンジングして不要なデータを除きましょう\n",
    "# sortは並び替えです（デフォルトでは昇順になります）\n",
    "\n",
    "df.where(df.gengo == \"平成\").groupBy(\"kenmei\") \\\n",
    "  .agg(sf.avg(\"jinko_male\").alias(\"male_avg\"),sf.avg(\"jinko_female\").alias(\"female_avg\")) \\\n",
    "    .filter(df.kenmei != \"人口集中地区以外の地区\").sort(\"male_avg\").show()\n",
    "\n",
    "#良さそうです！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#結果を一度保存しておきます\n",
    "df_after_t=df.where(df.gengo == \"平成\").groupBy(\"kenmei\") \\\n",
    "  .agg(sf.avg(\"jinko_male\").alias(\"male_avg\"),sf.avg(\"jinko_female\").alias(\"female_avg\")) \\\n",
    "  .filter(df.kenmei != \"人口集中地区以外の地区\").sort(\"male_avg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIKWモデル\n",
    "![図1.2 DIKW](images/DIKW.png)\n",
    "少し脇道にそれるのですが、上記の作業はDIKWモデルというものに沿った動きでです。\n",
    "\n",
    "DIKWモデルでは、データのステージを「Data」「Infromation」「Knowledge」「Wisdom」として定義しています。\n",
    "\n",
    "- Data(データ)\n",
    "- Information（情報）\n",
    "- Knowledge（知識）\n",
    "- Wisdom（知恵）\n",
    "\n",
    "これらの頭文字をとってDIKWモデルと呼ばれています。\n",
    "\n",
    "ETLをすることはDataを情報や知識に変換することを指します。\n",
    "情報や知識は、データから見つかるルールや関係性のことです。\n",
    "今回の場合だと、鳥取県が人口少ないです　という事実がわかったという形になります。\n",
    "\n",
    "知恵は、この知識から生み出すもので、例えば鳥取県の人口が少なく、それが問題なのであれば\n",
    "その問題を解決するための施策が知恵になります。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# カラムナーフォーマットへ変換する\n",
    "データの変換が終わったので次は、そのデータをビッグデータ向けのフォーマットで保存することを考えます。  \n",
    "今回はParquet形式へデータを変換します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/11 17:06:55 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n"
     ]
    }
   ],
   "source": [
    "#単純に吐き出す方法\n",
    "df_after_t.write.mode(\"overwrite\").parquet(\"/Users/yuki/pyspark_batch/dataset/parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 792\n",
      "drwxr-xr-x  102 yuki  staff  3264 Oct 11 17:07 \u001b[34m.\u001b[m\u001b[m\n",
      "drwxr-xr-x    4 yuki  staff   128 Oct 11 17:06 \u001b[34m..\u001b[m\u001b[m\n",
      "-rw-r--r--    1 yuki  staff     8 Oct 11 17:07 ._SUCCESS.crc\n",
      "-rw-r--r--    1 yuki  staff    16 Oct 11 17:06 .part-00000-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet.crc\n",
      "-rw-r--r--    1 yuki  staff    16 Oct 11 17:06 .part-00001-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet.crc\n",
      "-rw-r--r--    1 yuki  staff    16 Oct 11 17:06 .part-00002-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet.crc\n",
      "-rw-r--r--    1 yuki  staff    16 Oct 11 17:06 .part-00003-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet.crc\n",
      "-rw-r--r--    1 yuki  staff    16 Oct 11 17:06 .part-00004-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet.crc\n",
      "-rw-r--r--    1 yuki  staff    16 Oct 11 17:06 .part-00005-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet.crc\n",
      "-rw-r--r--    1 yuki  staff    16 Oct 11 17:06 .part-00006-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet.crc\n",
      "-rw-r--r--    1 yuki  staff    16 Oct 11 17:06 .part-00007-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet.crc\n",
      "-rw-r--r--    1 yuki  staff    20 Oct 11 17:06 .part-00008-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet.crc\n",
      "-rw-r--r--    1 yuki  staff    16 Oct 11 17:06 .part-00009-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet.crc\n",
      "-rw-r--r--    1 yuki  staff    16 Oct 11 17:06 .part-00010-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet.crc\n",
      "-rw-r--r--    1 yuki  staff    16 Oct 11 17:06 .part-00011-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet.crc\n",
      "-rw-r--r--    1 yuki  staff    16 Oct 11 17:06 .part-00012-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet.crc\n",
      "-rw-r--r--    1 yuki  staff    16 Oct 11 17:06 .part-00013-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet.crc\n",
      "-rw-r--r--    1 yuki  staff    16 Oct 11 17:06 .part-00014-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet.crc\n",
      "-rw-r--r--    1 yuki  staff    16 Oct 11 17:06 .part-00015-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet.crc\n",
      "-rw-r--r--    1 yuki  staff    16 Oct 11 17:06 .part-00016-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet.crc\n",
      "-rw-r--r--    1 yuki  staff    16 Oct 11 17:06 .part-00017-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet.crc\n",
      "-rw-r--r--    1 yuki  staff    16 Oct 11 17:06 .part-00018-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet.crc\n",
      "-rw-r--r--    1 yuki  staff    16 Oct 11 17:06 .part-00019-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet.crc\n",
      "-rw-r--r--    1 yuki  staff    16 Oct 11 17:06 .part-00020-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet.crc\n",
      "-rw-r--r--    1 yuki  staff    16 Oct 11 17:06 .part-00021-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet.crc\n",
      "-rw-r--r--    1 yuki  staff    16 Oct 11 17:06 .part-00022-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet.crc\n",
      "-rw-r--r--    1 yuki  staff    20 Oct 11 17:06 .part-00023-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet.crc\n",
      "-rw-r--r--    1 yuki  staff    16 Oct 11 17:06 .part-00024-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet.crc\n",
      "-rw-r--r--    1 yuki  staff    16 Oct 11 17:06 .part-00025-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet.crc\n",
      "-rw-r--r--    1 yuki  staff    16 Oct 11 17:06 .part-00026-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet.crc\n",
      "-rw-r--r--    1 yuki  staff    16 Oct 11 17:06 .part-00027-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet.crc\n",
      "-rw-r--r--    1 yuki  staff    16 Oct 11 17:06 .part-00028-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet.crc\n",
      "-rw-r--r--    1 yuki  staff    16 Oct 11 17:06 .part-00029-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet.crc\n",
      "-rw-r--r--    1 yuki  staff    16 Oct 11 17:06 .part-00030-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet.crc\n",
      "-rw-r--r--    1 yuki  staff    16 Oct 11 17:06 .part-00031-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet.crc\n",
      "-rw-r--r--    1 yuki  staff    16 Oct 11 17:06 .part-00032-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet.crc\n",
      "-rw-r--r--    1 yuki  staff    16 Oct 11 17:06 .part-00033-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet.crc\n",
      "-rw-r--r--    1 yuki  staff    16 Oct 11 17:06 .part-00034-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet.crc\n",
      "-rw-r--r--    1 yuki  staff    16 Oct 11 17:06 .part-00035-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet.crc\n",
      "-rw-r--r--    1 yuki  staff    16 Oct 11 17:06 .part-00036-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet.crc\n",
      "-rw-r--r--    1 yuki  staff    16 Oct 11 17:06 .part-00037-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet.crc\n",
      "-rw-r--r--    1 yuki  staff    16 Oct 11 17:06 .part-00038-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet.crc\n",
      "-rw-r--r--    1 yuki  staff    16 Oct 11 17:06 .part-00039-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet.crc\n",
      "-rw-r--r--    1 yuki  staff    16 Oct 11 17:07 .part-00040-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet.crc\n",
      "-rw-r--r--    1 yuki  staff    16 Oct 11 17:07 .part-00041-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet.crc\n",
      "-rw-r--r--    1 yuki  staff    16 Oct 11 17:07 .part-00042-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet.crc\n",
      "-rw-r--r--    1 yuki  staff    16 Oct 11 17:07 .part-00043-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet.crc\n",
      "-rw-r--r--    1 yuki  staff    16 Oct 11 17:07 .part-00044-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet.crc\n",
      "-rw-r--r--    1 yuki  staff    20 Oct 11 17:07 .part-00045-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet.crc\n",
      "-rw-r--r--    1 yuki  staff    16 Oct 11 17:07 .part-00046-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet.crc\n",
      "-rw-r--r--    1 yuki  staff    20 Oct 11 17:07 .part-00047-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet.crc\n",
      "-rw-r--r--    1 yuki  staff    16 Oct 11 17:07 .part-00048-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet.crc\n",
      "-rw-r--r--    1 yuki  staff     0 Oct 11 17:07 _SUCCESS\n",
      "-rw-r--r--    1 yuki  staff  1001 Oct 11 17:06 part-00000-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet\n",
      "-rw-r--r--    1 yuki  staff  1001 Oct 11 17:06 part-00001-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet\n",
      "-rw-r--r--    1 yuki  staff  1001 Oct 11 17:06 part-00002-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet\n",
      "-rw-r--r--    1 yuki  staff  1001 Oct 11 17:06 part-00003-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet\n",
      "-rw-r--r--    1 yuki  staff  1001 Oct 11 17:06 part-00004-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet\n",
      "-rw-r--r--    1 yuki  staff  1001 Oct 11 17:06 part-00005-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet\n",
      "-rw-r--r--    1 yuki  staff  1001 Oct 11 17:06 part-00006-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet\n",
      "-rw-r--r--    1 yuki  staff  1001 Oct 11 17:06 part-00007-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet\n",
      "-rw-r--r--    1 yuki  staff  1028 Oct 11 17:06 part-00008-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet\n",
      "-rw-r--r--    1 yuki  staff  1001 Oct 11 17:06 part-00009-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet\n",
      "-rw-r--r--    1 yuki  staff  1001 Oct 11 17:06 part-00010-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet\n",
      "-rw-r--r--    1 yuki  staff  1001 Oct 11 17:06 part-00011-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet\n",
      "-rw-r--r--    1 yuki  staff  1001 Oct 11 17:06 part-00012-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet\n",
      "-rw-r--r--    1 yuki  staff  1001 Oct 11 17:06 part-00013-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet\n",
      "-rw-r--r--    1 yuki  staff  1001 Oct 11 17:06 part-00014-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet\n",
      "-rw-r--r--    1 yuki  staff  1001 Oct 11 17:06 part-00015-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet\n",
      "-rw-r--r--    1 yuki  staff  1001 Oct 11 17:06 part-00016-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet\n",
      "-rw-r--r--    1 yuki  staff  1001 Oct 11 17:06 part-00017-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet\n",
      "-rw-r--r--    1 yuki  staff  1001 Oct 11 17:06 part-00018-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet\n",
      "-rw-r--r--    1 yuki  staff  1001 Oct 11 17:06 part-00019-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet\n",
      "-rw-r--r--    1 yuki  staff  1001 Oct 11 17:06 part-00020-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet\n",
      "-rw-r--r--    1 yuki  staff  1001 Oct 11 17:06 part-00021-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet\n",
      "-rw-r--r--    1 yuki  staff  1001 Oct 11 17:06 part-00022-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet\n",
      "-rw-r--r--    1 yuki  staff  1028 Oct 11 17:06 part-00023-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet\n",
      "-rw-r--r--    1 yuki  staff  1001 Oct 11 17:06 part-00024-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet\n",
      "-rw-r--r--    1 yuki  staff  1001 Oct 11 17:06 part-00025-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet\n",
      "-rw-r--r--    1 yuki  staff  1001 Oct 11 17:06 part-00026-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet\n",
      "-rw-r--r--    1 yuki  staff  1001 Oct 11 17:06 part-00027-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet\n",
      "-rw-r--r--    1 yuki  staff  1001 Oct 11 17:06 part-00028-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet\n",
      "-rw-r--r--    1 yuki  staff  1001 Oct 11 17:06 part-00029-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet\n",
      "-rw-r--r--    1 yuki  staff  1001 Oct 11 17:06 part-00030-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet\n",
      "-rw-r--r--    1 yuki  staff  1001 Oct 11 17:06 part-00031-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet\n",
      "-rw-r--r--    1 yuki  staff  1001 Oct 11 17:06 part-00032-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet\n",
      "-rw-r--r--    1 yuki  staff  1001 Oct 11 17:06 part-00033-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet\n",
      "-rw-r--r--    1 yuki  staff  1001 Oct 11 17:06 part-00034-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet\n",
      "-rw-r--r--    1 yuki  staff  1001 Oct 11 17:06 part-00035-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet\n",
      "-rw-r--r--    1 yuki  staff  1001 Oct 11 17:06 part-00036-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet\n",
      "-rw-r--r--    1 yuki  staff  1001 Oct 11 17:06 part-00037-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet\n",
      "-rw-r--r--    1 yuki  staff  1001 Oct 11 17:06 part-00038-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet\n",
      "-rw-r--r--    1 yuki  staff  1001 Oct 11 17:06 part-00039-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet\n",
      "-rw-r--r--    1 yuki  staff  1001 Oct 11 17:07 part-00040-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet\n",
      "-rw-r--r--    1 yuki  staff  1001 Oct 11 17:07 part-00041-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet\n",
      "-rw-r--r--    1 yuki  staff  1001 Oct 11 17:07 part-00042-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet\n",
      "-rw-r--r--    1 yuki  staff  1001 Oct 11 17:07 part-00043-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet\n",
      "-rw-r--r--    1 yuki  staff  1001 Oct 11 17:07 part-00044-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet\n",
      "-rw-r--r--    1 yuki  staff  1028 Oct 11 17:07 part-00045-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet\n",
      "-rw-r--r--    1 yuki  staff  1001 Oct 11 17:07 part-00046-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet\n",
      "-rw-r--r--    1 yuki  staff  1082 Oct 11 17:07 part-00047-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet\n",
      "-rw-r--r--    1 yuki  staff   974 Oct 11 17:07 part-00048-2ec6caaf-10ee-4688-a37f-71779e20c13a-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "#ファイルを見てみます\n",
    "!ls -al /Users/yuki/pyspark_batch/dataset/parquet\n",
    "\n",
    "#　確かにParquetnoファイルが出力されていますが、ファイルが多いですね。。 一つのファイルサイズが小さいのも気になります"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# スモールファイル問題\n",
    "オンプレでもクラウドでもそうなのですが、ビッグデータの世界では一つのファイルが小さすぎると途端に処理が遅くなります。  \n",
    "この問題をスモールファイル問題と読んでいます。  \n",
    "一般に、１GBくらいずつまとめることが推奨されています。\n",
    "\n",
    "この問題を解決するためには、repartition(もしくはcolaese)を使ってファイルをマージする必要があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# 今回はファイルを一個に纏めてみようと思います。\n",
    "df_after_t.repartition(1).write.mode(\"overwrite\").parquet(\"/Users/yuki/pyspark_batch/dataset/parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 8\n",
      "-rw-r--r--  1 yuki  staff     0 Oct 11 17:12 _SUCCESS\n",
      "-rw-r--r--  1 yuki  staff  1990 Oct 11 17:12 part-00000-5a6c078e-ff20-44c8-96a5-276c2ac5eee0-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "# もう一度ファイルを見てみます\n",
    "!ls -l /Users/yuki/pyspark_batch/dataset/parquet\n",
    "# 一つになりました！ スモールファイル問題も解決です。\n",
    "#　今回は一個にまとめましたが、一個にまとめるのが常に最適とは限りません。一つのサイズは１G~５GBくらいにするといいと言われています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "#もう少し書き込みのオプションを見ていきます\n",
    "# partitionByを使うことで、データをパーティションごと(次に説明します)に分けて配置することができます。\n",
    "# 今回はkenmei(県名)ごとにデータを保存してみようと思います。\n",
    "df_after_t.repartition(1).write.partitionBy(\"kenmei\").mode(\"overwrite\").parquet(\"/Users/yuki/pyspark_batch/dataset/parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0\n",
      "-rw-r--r--  1 yuki  staff    0 Oct 11 17:14 _SUCCESS\n",
      "drwxr-xr-x  4 yuki  staff  128 Oct 11 17:14 \u001b[34mkenmei=三重県\u001b[m\u001b[m\n",
      "drwxr-xr-x  4 yuki  staff  128 Oct 11 17:14 \u001b[34mkenmei=京都府\u001b[m\u001b[m\n",
      "drwxr-xr-x  4 yuki  staff  128 Oct 11 17:14 \u001b[34mkenmei=人口集中地区\u001b[m\u001b[m\n",
      "drwxr-xr-x  4 yuki  staff  128 Oct 11 17:14 \u001b[34mkenmei=佐賀県\u001b[m\u001b[m\n",
      "drwxr-xr-x  4 yuki  staff  128 Oct 11 17:14 \u001b[34mkenmei=全国\u001b[m\u001b[m\n",
      "drwxr-xr-x  4 yuki  staff  128 Oct 11 17:14 \u001b[34mkenmei=兵庫県\u001b[m\u001b[m\n",
      "drwxr-xr-x  4 yuki  staff  128 Oct 11 17:14 \u001b[34mkenmei=北海道\u001b[m\u001b[m\n",
      "drwxr-xr-x  4 yuki  staff  128 Oct 11 17:14 \u001b[34mkenmei=千葉県\u001b[m\u001b[m\n",
      "drwxr-xr-x  4 yuki  staff  128 Oct 11 17:14 \u001b[34mkenmei=和歌山県\u001b[m\u001b[m\n",
      "drwxr-xr-x  4 yuki  staff  128 Oct 11 17:14 \u001b[34mkenmei=埼玉県\u001b[m\u001b[m\n",
      "drwxr-xr-x  4 yuki  staff  128 Oct 11 17:14 \u001b[34mkenmei=大分県\u001b[m\u001b[m\n",
      "drwxr-xr-x  4 yuki  staff  128 Oct 11 17:14 \u001b[34mkenmei=大阪府\u001b[m\u001b[m\n",
      "drwxr-xr-x  4 yuki  staff  128 Oct 11 17:14 \u001b[34mkenmei=奈良県\u001b[m\u001b[m\n",
      "drwxr-xr-x  4 yuki  staff  128 Oct 11 17:14 \u001b[34mkenmei=宮城県\u001b[m\u001b[m\n",
      "drwxr-xr-x  4 yuki  staff  128 Oct 11 17:14 \u001b[34mkenmei=宮崎県\u001b[m\u001b[m\n",
      "drwxr-xr-x  4 yuki  staff  128 Oct 11 17:14 \u001b[34mkenmei=富山県\u001b[m\u001b[m\n",
      "drwxr-xr-x  4 yuki  staff  128 Oct 11 17:14 \u001b[34mkenmei=山口県\u001b[m\u001b[m\n",
      "drwxr-xr-x  4 yuki  staff  128 Oct 11 17:14 \u001b[34mkenmei=山形県\u001b[m\u001b[m\n",
      "drwxr-xr-x  4 yuki  staff  128 Oct 11 17:14 \u001b[34mkenmei=山梨県\u001b[m\u001b[m\n",
      "drwxr-xr-x  4 yuki  staff  128 Oct 11 17:14 \u001b[34mkenmei=岐阜県\u001b[m\u001b[m\n",
      "drwxr-xr-x  4 yuki  staff  128 Oct 11 17:14 \u001b[34mkenmei=岡山県\u001b[m\u001b[m\n",
      "drwxr-xr-x  4 yuki  staff  128 Oct 11 17:14 \u001b[34mkenmei=岩手県\u001b[m\u001b[m\n",
      "drwxr-xr-x  4 yuki  staff  128 Oct 11 17:14 \u001b[34mkenmei=島根県\u001b[m\u001b[m\n",
      "drwxr-xr-x  4 yuki  staff  128 Oct 11 17:14 \u001b[34mkenmei=広島県\u001b[m\u001b[m\n",
      "drwxr-xr-x  4 yuki  staff  128 Oct 11 17:14 \u001b[34mkenmei=徳島県\u001b[m\u001b[m\n",
      "drwxr-xr-x  4 yuki  staff  128 Oct 11 17:14 \u001b[34mkenmei=愛媛県\u001b[m\u001b[m\n",
      "drwxr-xr-x  4 yuki  staff  128 Oct 11 17:14 \u001b[34mkenmei=愛知県\u001b[m\u001b[m\n",
      "drwxr-xr-x  4 yuki  staff  128 Oct 11 17:14 \u001b[34mkenmei=新潟県\u001b[m\u001b[m\n",
      "drwxr-xr-x  4 yuki  staff  128 Oct 11 17:14 \u001b[34mkenmei=東京都\u001b[m\u001b[m\n",
      "drwxr-xr-x  4 yuki  staff  128 Oct 11 17:14 \u001b[34mkenmei=栃木県\u001b[m\u001b[m\n",
      "drwxr-xr-x  4 yuki  staff  128 Oct 11 17:14 \u001b[34mkenmei=沖縄県\u001b[m\u001b[m\n",
      "drwxr-xr-x  4 yuki  staff  128 Oct 11 17:14 \u001b[34mkenmei=滋賀県\u001b[m\u001b[m\n",
      "drwxr-xr-x  4 yuki  staff  128 Oct 11 17:14 \u001b[34mkenmei=熊本県\u001b[m\u001b[m\n",
      "drwxr-xr-x  4 yuki  staff  128 Oct 11 17:14 \u001b[34mkenmei=石川県\u001b[m\u001b[m\n",
      "drwxr-xr-x  4 yuki  staff  128 Oct 11 17:14 \u001b[34mkenmei=神奈川県\u001b[m\u001b[m\n",
      "drwxr-xr-x  4 yuki  staff  128 Oct 11 17:14 \u001b[34mkenmei=福井県\u001b[m\u001b[m\n",
      "drwxr-xr-x  4 yuki  staff  128 Oct 11 17:14 \u001b[34mkenmei=福岡県\u001b[m\u001b[m\n",
      "drwxr-xr-x  4 yuki  staff  128 Oct 11 17:14 \u001b[34mkenmei=福島県\u001b[m\u001b[m\n",
      "drwxr-xr-x  4 yuki  staff  128 Oct 11 17:14 \u001b[34mkenmei=秋田県\u001b[m\u001b[m\n",
      "drwxr-xr-x  4 yuki  staff  128 Oct 11 17:14 \u001b[34mkenmei=群馬県\u001b[m\u001b[m\n",
      "drwxr-xr-x  4 yuki  staff  128 Oct 11 17:14 \u001b[34mkenmei=茨城県\u001b[m\u001b[m\n",
      "drwxr-xr-x  4 yuki  staff  128 Oct 11 17:14 \u001b[34mkenmei=長崎県\u001b[m\u001b[m\n",
      "drwxr-xr-x  4 yuki  staff  128 Oct 11 17:14 \u001b[34mkenmei=長野県\u001b[m\u001b[m\n",
      "drwxr-xr-x  4 yuki  staff  128 Oct 11 17:14 \u001b[34mkenmei=青森県\u001b[m\u001b[m\n",
      "drwxr-xr-x  4 yuki  staff  128 Oct 11 17:14 \u001b[34mkenmei=静岡県\u001b[m\u001b[m\n",
      "drwxr-xr-x  4 yuki  staff  128 Oct 11 17:14 \u001b[34mkenmei=香川県\u001b[m\u001b[m\n",
      "drwxr-xr-x  4 yuki  staff  128 Oct 11 17:14 \u001b[34mkenmei=高知県\u001b[m\u001b[m\n",
      "drwxr-xr-x  4 yuki  staff  128 Oct 11 17:14 \u001b[34mkenmei=鳥取県\u001b[m\u001b[m\n",
      "drwxr-xr-x  4 yuki  staff  128 Oct 11 17:14 \u001b[34mkenmei=鹿児島県\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "# もう一度ファイルを見てみます\n",
    "!ls -l /Users/yuki/pyspark_batch/dataset/parquet\n",
    "\n",
    "#県名が出てきました\n",
    "#三重県のデータはkenmei=三重県の下に格納されています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 8\n",
      "-rw-r--r--  1 yuki  staff  741 Oct 11 17:14 part-00000-4be275c5-908f-4d9b-9ea7-d337a576284b.c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "#　三重県のデータを見てみます\n",
    "!ls -l /Users/yuki/pyspark_batch/dataset/parquet/kenmei=三重県"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+\n",
      "|male_avg|       female_avg|\n",
      "+--------+-----------------+\n",
      "|893167.5|944959.6666666666|\n",
      "+--------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# ここでSparkでParquetのデータを読み込んでみます\n",
    "\n",
    "parquet_df=spark.read.parquet(\"/Users/yuki/pyspark_batch/dataset/parquet/kenmei=三重県\")\n",
    "parquet_df.show()\n",
    "\n",
    "#コレが三重県の男性と女性の人口の平均値です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 出力したデータをみんなに見やすくするため(BIツールから参照できるように)テーブルを作成する\n",
    "次はテーブルの作成を行ってみようと思います。  \n",
    "今のままだとエンジニア向けでちょっと使い勝手が悪いのとBIツールといった他のツールから参照することができません"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Createテーブルを発行します\n",
    "# 次のチャプターでも紹介しますが、spark.sqlという関数を使います\n",
    "\n",
    "#jinko_avgテーブルを作成します。\n",
    "#パーティションとはデータを分けるフォルダみたいなもの、パーティションを分けることで読み込むデータ量を少なくしたりできるので最適化できる\n",
    "#先程確認したkenmei=の部分がパーティションになっているのでその出力結果に合わせてテーブルを作成してみます。\n",
    "\n",
    "#ロケーションは、kenmeiを含まずに指定します。\n",
    "\n",
    "spark.sql(\"\"\" \n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS default.jinko_avg ( male_avg double, female_avg double)\n",
    "PARTITIONED BY (kenmei String)\n",
    "STORED AS PARQUET\n",
    "LOCATION '/Users/yuki/pyspark_batch/dataset/parquet/';\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------+\n",
      "|database| tableName|isTemporary|\n",
      "+--------+----------+-----------+\n",
      "| default| jinko_avg|      false|\n",
      "| default|jinko_avg2|      false|\n",
      "+--------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# テーブルを見てみます。\n",
    "spark.sql(\"show tables\").show()\n",
    "\n",
    "# ちゃんとできているようですね。\n",
    "# ちなみに実行しているSQLは実はSQLみたいなものでHiveSQLと呼ばれるものです。\n",
    "# Mysqlの扱いとほとんど同じなので、Mysqlみたいに使って動かなかったところだけ検索すると効率が良いと思います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+\n",
      "|male_avg|female_avg|kenmei|\n",
      "+--------+----------+------+\n",
      "+--------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#spark.sql()はdataframeを戻り値として返してくれます\n",
    "\n",
    "df_result=spark.sql(\"select * from default.jinko_avg\")\n",
    "df_result.show()\n",
    "\n",
    "#おや。データを見ることができません。。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# テーブルだけでなく、Partitionを認識させてあげないといけません\n",
    "# msck repair table　テーブル名と実行するとパーティションが認識されます(ちなみにAdd partitionというコマンドもあります)。\n",
    "spark.sql(\"msck repair table jinko_avg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------+\n",
      "|            male_avg|          female_avg|      kenmei|\n",
      "+--------------------+--------------------+------------+\n",
      "|            893167.5|   944959.6666666666|      三重県|\n",
      "|  1268325.3333333333|  1360099.3333333333|      京都府|\n",
      "|4.0840519833333336E7|4.2415789666666664E7|人口集中地区|\n",
      "|            408192.5|            456442.5|      佐賀県|\n",
      "|         6.1816723E7|6.4687834833333336E7|        全国|\n",
      "|           2650310.5|           2861527.0|      兵庫県|\n",
      "|  2665781.3333333335|  2923371.8333333335|      北海道|\n",
      "|           2987847.0|           2974638.5|      千葉県|\n",
      "|            490624.0|   547112.3333333334|    和歌山県|\n",
      "|  3492880.3333333335|  3443447.8333333335|      埼玉県|\n",
      "|   571530.6666666666|   638773.6666666666|      大分県|\n",
      "|   4292675.833333333|           4517115.0|      大阪府|\n",
      "|   671178.6666666666|   734736.6666666666|      奈良県|\n",
      "|           1139561.5|           1191255.0|      宮城県|\n",
      "|            542386.5|            608793.0|      宮崎県|\n",
      "|            532857.0|   573049.8333333334|      富山県|\n",
      "|   709497.8333333334|            791301.5|      山口県|\n",
      "|            583603.5|   627811.6666666666|      山形県|\n",
      "|   425777.8333333333|   441831.1666666667|      山梨県|\n",
      "|  1009389.1666666666|  1073025.1666666667|      岐阜県|\n",
      "+--------------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# 今一度検索をしてみます。\n",
    "spark.sql(\"select * from default.jinko_avg\").show()\n",
    "#　今度は出ましたね！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+------+\n",
      "|         male_avg|       female_avg|kenmei|\n",
      "+-----------------+-----------------+------+\n",
      "|6222455.666666667|6268299.166666667|東京都|\n",
      "+-----------------+-----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# もちろんSQLなので whereも可能です\n",
    "spark.sql(\"select * from default.jinko_avg where kenmei='東京都'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 次のチャプターはSparkSQLについて紹介していこうと思います"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark利用の停止\n",
    "spark.stop()\n",
    "spark.sparkContext.stop()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cadefa720d1a2267f4d12d08d812560a64cfe891877bc388bf0e8af3e4846067"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
