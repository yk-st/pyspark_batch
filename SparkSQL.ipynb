{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.0 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "cadefa720d1a2267f4d12d08d812560a64cfe891877bc388bf0e8af3e4846067"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pysparkに必要なライブラリを読み込む\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#spark sessionの作成\n",
    "# spark.ui.enabled trueとするとSparkのGUI画面を確認することができます\n",
    "# spark.eventLog.enabled true　とすると　GUIで実行ログを確認することができます\n",
    "# GUIなどの確認は次のチャプターで説明を行います。\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"chapter2\") \\\n",
    "    .config(\"hive.exec.dynamic.partition\", \"true\") \\\n",
    "    .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"JST\") \\\n",
    "    .config(\"spark.ui.enabled\",\"true\") \\\n",
    "    .config(\"spark.eventLog.enabled\",\"true\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "source": [
    "# 前回のチャプターいかがでしたか？\n",
    "ちょっとdataframeの操作は慣れてなくて。。。\n",
    "という人多かったんじゃないでしょうか\n",
    "\n",
    "今回はデータをSQLで操作可能なSparkSQLを使いながら前回のチャプターと同じことをやっていこうと思います"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Sparkを用いたバッチにおけるデータエンジニアリング一連の流れ(復習)\n",
    "\n",
    "1. データソースの読み込み(今回は、人口統計データ(/dataset/jinkou.csv))　ETL(Extract Transform Load)で言うEの部分\n",
    "2. 変換を行う(集計等を行う)　DataFrame処理 or SQL処理の２パターンで実行可能 ETL(Extract Transform Load)で言うTの部分\n",
    "3. カラムナーフォーマットへ変換する ETL(Extract Transform Load)で言うTの部分\n",
    "4. 出力したデータをみんなに見やすくするため(BIツールから参照できるように)テーブルを作成する ETL(Extract Transform Load)で言うLの部分\n",
    "\n",
    "よくある、関数の羅列をするのではなく、実業務に沿った形で流れを紹介していきます。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# データソースの読み込み"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import LongType, StructType, StructField, StringType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# カラム名、型、デフォルト値で設定していきます\n",
    "struct = StructType([\n",
    "    StructField(\"code\", StringType(), False),\n",
    "    StructField(\"kenmei\", StringType(), False),\n",
    "    StructField(\"gengo\", StringType(), False),\n",
    "    StructField(\"wareki\", StringType(), False),\n",
    "    StructField(\"seireki\", StringType(), False),\n",
    "    StructField(\"chu\", StringType(), False),\n",
    "    StructField(\"jinko_male\", StringType(), False),\n",
    "    StructField(\"jinko_female\", StringType(), False)\n",
    "])\n",
    "df=spark.read.option(\"multiLine\", \"true\").option(\"encoding\", \"SJIS\").csv(\"/Users/yuki/pyspark_batch/dataset/jinko.csv\", header=False, sep=',', inferSchema=False,schema=struct)"
   ]
  },
  {
   "source": [
    "# 変換を行う(集計等を行う)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ここでDataFrameの処理と違いSQLの場合は、仮想的なテーブルjinkoを作成します\n",
    "# テーブルを作成することでSQLを発行することができるようになります\n",
    "df.createOrReplaceTempView(\"jinko\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 早速SQLを実行してみます\n",
    "spark.sql(\"select * from jinko\").show()"
   ]
  },
  {
   "source": [
    "# 前のチャプターでDataframeベースで実行していた処理をSparkSQLを使って実装し直してみます。\n",
    "\n",
    "```\n",
    "df_after_t=df.where(df.gengo == \"平成\").groupBy(\"kenmei\").agg(sf.avg(\"jinko_male\").alias(\"male_avg\"),sf.avg(\"jinko_female\").alias(\"female_avg\")).filter(df.kenmei != \"人口集中地区以外の地区\").sort(\"male_avg\")\n",
    "\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQLで書き直すと、等価な処理は以下になります。\n",
    "\n",
    "df_after_t=spark.sql(\"\"\" \n",
    "\n",
    "select kenmei,avg(jinko_male) as male_avg,avg(jinko_female) as female_avg \n",
    " from jinko\n",
    "  where gengo='平成' and kenmei != '人口集中地区以外の地区'\n",
    " group by kenmei\n",
    " order by male_avg\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "df_after_t.show()\n",
    "\n",
    "#後のカラムナーフォーマットへの出力からは前のチャプターと一緒です。"
   ]
  },
  {
   "source": [
    "# SparkSQLを使うメリット\n",
    "\n",
    "- SQLを使った処理はSQLファイルを外部に配置してそのファイルを読み込み実行することで汎用化がしやすい\n",
    "- SQLになれた人であれば操作がしやすい\n",
    "- データエンジニアリングとしては、SparkSQLを使ってシステム構築をするほうが汎用的で容易\n",
    "\n",
    "# DataFrameを使うメリット\n",
    "- データフレームにしかできないような仕事もある。例えば行列の変換などはdataframeのほうが実行しやすいのでデータサイエンスを好む人はDataFrameを使うほうが良い場合もある。\n",
    "\n",
    "## 速度は変わる？\n",
    "速度はどちらでも変わりません。内部的にはSQLはDataFrameとして処理され実行されます。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}