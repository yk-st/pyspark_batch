{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![図1.2 Sparkの紹介とインストール](images/PysparkSQL.png)\n",
    "\n",
    "# 目次\n",
    "1. 前のセクションの振り返りとSparkSQLの紹介\n",
    "2. SparkSQLを用いてデータエンジニアリング一連の流れを再現\n",
    "3. テーブル定義はどこに保存されている？\n",
    "4. SparkSQLの他のTips(Insert文)\n",
    "5. SparkSQLの他のTips(Hint文)\n",
    "6. Update/Delete文？\n",
    "7. SparkSQLを使うメリット、Dataframeを使うメリット\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前回のセクションいかがでしたか？\n",
    "前回のセクションでは、DataFrameを用いてデータの変換(ETLにおけるTの処理)を行いました。\n",
    "\n",
    "しかし、ちょっとdataframeの操作は慣れてなくて。。。\n",
    "という人多かったんじゃないでしょうか\n",
    "\n",
    "```\n",
    "df_after_t=df.where(df.gengo == \"平成\").groupBy(\"kenmei\") \\\n",
    "    .agg(sf.avg(\"jinko_male\").alias(\"male_avg\"),sf.avg(\"jinko_female\").alias(\"female_avg\")) \\\n",
    "      .filter(df.kenmei != \"人口集中地区以外の地区\").sort(\"male_avg\")\n",
    "```\n",
    "\n",
    "今回はデータをSQLで操作可能なSparkSQLを使いながら前回のチャプターと同じことをやっていこうと思います\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkSQLとは？\n",
    "Apache HiveベースのSQLを実行できます(Mysqlの使い勝手に似ています。そのため、Mysqlの感覚で使ってみて使えないところをピンポイントで検索するといいと思います。)。  \n",
    "データフレームの操作が苦手な人でも、SQLを知っていればSparkで読み込んだデータを操作してT（変換処理）が可能になります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkを用いたバッチにおけるデータエンジニアリング一連の流れ(復習)\n",
    "\n",
    "1. データソースの読み込み(今回は、人口統計データ(/dataset/jinkou.csv))　ETL(Extract Transform Load)で言うEの部分\n",
    "2. 変換を行う(集計等を行う)　DataFrame処理 or SQL処理の２パターンで実行可能 ETL(Extract Transform Load)で言うTの部分\n",
    "3. カラムナーフォーマットへ変換する ETL(Extract Transform Load)で言うTの部分\n",
    "4. 出力したデータをみんなに見やすくするため(BIツールから参照できるように)テーブルを作成する ETL(Extract Transform Load)で言うLの部分\n",
    "\n",
    "よくある、関数の羅列をするのではなく、実業務に沿った形で流れを紹介していきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pysparkに必要なライブラリを読み込む\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#spark sessionの作成\n",
    "# spark.ui.enabled trueとするとSparkのGUI画面を確認することができます\n",
    "# spark.eventLog.enabled true　とすると　GUIで実行ログを確認することができます\n",
    "# GUIなどの確認は次のチャプターで説明を行います。\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"chapter2\") \\\n",
    "    .config(\"hive.exec.dynamic.partition\", \"true\") \\\n",
    "    .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"JST\") \\\n",
    "    .config(\"spark.ui.enabled\",\"true\") \\\n",
    "    .config(\"spark.eventLog.enabled\",\"true\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データソースの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import LongType, StructType, StructField, StringType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# カラム名、型、デフォルト値で設定していきます\n",
    "struct = StructType([\n",
    "    StructField(\"code\", StringType(), False),\n",
    "    StructField(\"kenmei\", StringType(), False),\n",
    "    StructField(\"gengo\", StringType(), False),\n",
    "    StructField(\"wareki\", StringType(), False),\n",
    "    StructField(\"seireki\", StringType(), False),\n",
    "    StructField(\"chu\", StringType(), False),\n",
    "    StructField(\"sokei\", StringType(), False),\n",
    "    StructField(\"jinko_male\", StringType(), False),\n",
    "    StructField(\"jinko_female\", StringType(), False)\n",
    "])\n",
    "df=spark.read.option(\"multiLine\", \"true\") \\\n",
    "    .option(\"encoding\", \"SJIS\").csv(\"/Users/yuki/pyspark_batch/dataset/jinko.csv\", header=False, sep=',', inferSchema=False,schema=struct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 変換を行う(集計等を行う)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ここでDataFrameの処理と違いSQLの場合は、仮想的なテーブルjinkoを作成します\n",
    "# テーブルを作成することでSQLを発行することができるようになります\n",
    "df.createOrReplaceTempView(\"jinko\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 早速SQLを実行してみます\n",
    "spark.sql(\"select * from jinko\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前のチャプターでDataframeベースで実行していた処理をSparkSQLを使って実装し直してみます。\n",
    "\n",
    "```\n",
    "df_after_t=df.where(df.gengo == \"平成\").groupBy(\"kenmei\") \\\n",
    "    .agg(sf.avg(\"jinko_male\").alias(\"male_avg\"),sf.avg(\"jinko_female\").alias(\"female_avg\")) \\\n",
    "      .filter(df.kenmei != \"人口集中地区以外の地区\").sort(\"male_avg\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQLで書き直すと、等価な処理は以下になります。\n",
    "\n",
    "df_after_t=spark.sql(\"\"\" \n",
    "\n",
    "select kenmei,avg(jinko_male) as male_avg,avg(jinko_female) as female_avg \n",
    " from jinko\n",
    "  where gengo='平成' and kenmei != '人口集中地区以外の地区'\n",
    " group by kenmei\n",
    " order by male_avg\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "df_after_t.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# テーブル定義はどこに保存されている？\n",
    "\n",
    "1. テンポラリーテーブル（メモリ）\n",
    "2. Create External TABLE（メタデータストア）\n",
    "   1. ローカルの場合metastore_db\n",
    "   2. 本番環境などの場合は、自前で構築する場合はMysql。クラウドサービスであればAmazon Glue Data Catalogなどがあります。\n",
    "\n",
    "今回はローカル環境なのでmetastore_dbと呼ばれるところに格納されています。\n",
    "永続的に保存されるので、前のチャプターで以下のデータベースを作っている人は定義が残っているはずです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#　テーブルの作成\n",
    "spark.sql(\"\"\" \n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS default.jinko_avg ( male_avg double, female_avg double)\n",
    "PARTITIONED BY (kenmei String)\n",
    "STORED AS PARQUET\n",
    "LOCATION '/Users/yuki/pyspark_batch/dataset/parquet/';\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert 文の発行\n",
    "SparkSQLでは Insert文も発行することが可能です  \n",
    "dataframeを吐き出してパーティションを認識させて。。というのは少し面倒  \n",
    "insert 文を使えばその作業を簡略化できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データの登録を行います\n",
    "\n",
    "次に紹介するspark sqlを用いたinsert(とselect)は前セクションで紹介した以下のコード実行と等価です。\n",
    "\n",
    "```\n",
    "df_after_t=df.where(df.gengo == \"平成\").groupBy(\"kenmei\") \\\n",
    "    .agg(sf.avg(\"jinko_male\").alias(\"male_avg\"),sf.avg(\"jinko_female\").alias(\"female_avg\")) \\\n",
    "      .filter(df.kenmei != \"人口集中地区以外の地区\").sort(\"male_avg\")\n",
    "```\n",
    "\n",
    "```\n",
    "df_after_t.repartition(1).write.partitionBy(\"kenmei\").mode(\"overwrite\").parquet(\"/Users/yuki/pyspark_batch/dataset/parquet\")\n",
    "```\n",
    "\n",
    "```\n",
    "spark.sql(\"msck repair table jinko_avg\")\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# パーティションであるkenmeiは一番うしろに記載する。\n",
    "# dynamic partitionと呼ばれる機能で、kenmeiごとにパーティションを振り分けてくれます\n",
    "dataframe=spark.sql(\"\"\" \n",
    "Insert overwrite table jinko_avg PARTITION(kenmei)\n",
    "\n",
    "select avg(jinko_male) as male_avg,avg(jinko_female),kenmei as female_avg \n",
    " from jinko\n",
    "  where gengo='平成' and kenmei != '人口集中地区以外の地区'\n",
    " group by kenmei\n",
    " order by male_avg\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from jinko_avg\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 出来上がったファイル一度みてみましょう(ロケーションは「/Users/yuki/pyspark_batch/dataset/parquet」なのでロケーションはいかにデータが出ているはずです)\n",
    "!ls -l /Users/yuki/pyspark_batch/dataset/parquet/kenmei=三重県"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hint文\n",
    "SQLを実行するときでもスモールファイル問題に対応できるようにrepartitionに相当する機能が存在しています。\n",
    "\n",
    "それがヒント文です。\n",
    "\n",
    "```\n",
    "/** REPARTITION(25) */\n",
    "\n",
    "```\n",
    "\n",
    "Dataframeの操作では以下の部分です。  \n",
    "df_after_t.repartition(1)\n",
    "\n",
    "先程のinsert文は実はそのままでも動くのですが、スモールファイル問題を引き起こす原因にもなってしまいます。  \n",
    "そこでこのヒント文を使って書き直してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ヒント文をSelectの部分に埋め込無事でファイルのばらつきを抑える(数字は２とすれば、パーティションごとに２つのファイルができるYewYe)\n",
    "dataframe=spark.sql(\"\"\" \n",
    "Insert overwrite table jinko_avg PARTITION(kenmei)\n",
    "\n",
    "select /** REPARTITION(10) */ avg(jinko_male) as male_avg,avg(jinko_female),kenmei as female_avg \n",
    " from jinko\n",
    "  where gengo='平成' and kenmei != '人口集中地区以外の地区'\n",
    " group by kenmei\n",
    " order by male_avg\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 出来上がったファイル一度みてみましょう(ロケーションは「/Users/yuki/pyspark_batch/dataset/parquet」なのでロケーションはいかにデータが出ているはずです)\n",
    "!ls -l /Users/yuki/pyspark_batch/dataset/parquet/kenmei=三重県"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# update文やdelete文は？\n",
    "Update/delete文はありません。  \n",
    "ビッグデータの世界では、トランザクションシステムのように対象のレコードだけをピンポイントで更新する機能を有していないものが多いです。  \n",
    "Sparkもその考えからUpdate/delete文を持ち合わせていません。　　\n",
    "\n",
    "何兆というレコードから対象のレコードを探すことはかなり難しいのと、たくさんのデータソースがまじり合うデータ基盤では\n",
    "トランザクションシステムのように、PKを設定することが困難なことが挙げられます"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkSQLを使うメリット\n",
    "\n",
    "- SQLを使った処理はSQLファイルを外部に配置してそのファイルを読み込み実行することで汎用化がしやすい\n",
    "- SQLになれた人であれば操作がしやすい\n",
    "- データエンジニアリングとしては、SparkSQLを使ってシステム構築をするほうが汎用的で容易\n",
    "\n",
    "# DataFrameを使うメリット\n",
    "- データフレームにしかできないような仕事もある。例えば行列の変換などはdataframeのほうが実行しやすいのでデータサイエンスを好む人はDataFrameを使うほうが良い場合もある。\n",
    "\n",
    "## 速度は変わる？\n",
    "速度はどちらでも変わりません。内部的にはSQLはDataFrameとして処理され実行されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark利用の停止\n",
    "spark.stop()\n",
    "spark.sparkContext.stop()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cadefa720d1a2267f4d12d08d812560a64cfe891877bc388bf0e8af3e4846067"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
