{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![図1.2 Sparkの紹介とインストール](images/PysparkSQL.png)\n",
    "\n",
    "# 目次\n",
    "1. 前のセクションの振り返りとSparkSQLの紹介\n",
    "2. SparkSQLを用いてデータエンジニアリング一連の流れを再現\n",
    "3. テーブル定義はどこに保存されている？\n",
    "4. SparkSQLの他のTips(Insert文)\n",
    "5. SparkSQLの他のTips(Hint文)\n",
    "6. Update/Delete文？\n",
    "7. SparkSQLを使うメリット、Dataframeを使うメリット\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前回のセクションいかがでしたか？\n",
    "前回のセクションでは、DataFrameを用いてデータの変換(ETLにおけるTの処理)を行いました。\n",
    "\n",
    "しかし、ちょっとdataframeの操作は慣れてなくて。。。\n",
    "という人多かったんじゃないでしょうか\n",
    "\n",
    "```\n",
    "df_after_t=df.where(df.gengo == \"平成\").groupBy(\"kenmei\") \\\n",
    "    .agg(sf.avg(\"jinko_male\").alias(\"male_avg\"),sf.avg(\"jinko_female\").alias(\"female_avg\")) \\\n",
    "      .filter(df.kenmei != \"人口集中地区以外の地区\").sort(\"male_avg\")\n",
    "```\n",
    "\n",
    "今回はデータをSQLで操作可能なSparkSQLを使いながら前回のチャプターと同じことをやっていこうと思います\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkSQLとは？\n",
    "Apache HiveベースのSQLを実行できます(Mysqlの使い勝手に似ています。そのため、Mysqlの感覚で使ってみて使えないところをピンポイントで検索するといいと思います。)。  \n",
    "データフレームの操作が苦手な人でも、SQLを知っていればSparkで読み込んだデータを操作してT（変換処理）が可能になります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkを用いたバッチにおけるデータエンジニアリング一連の流れ(復習)\n",
    "\n",
    "1. データソースの読み込み(今回は、人口統計データ(/dataset/jinkou.csv))　ETL(Extract Transform Load)で言うEの部分\n",
    "2. 変換を行う(集計等を行う)　DataFrame処理 or SQL処理の２パターンで実行可能 ETL(Extract Transform Load)で言うTの部分\n",
    "3. カラムナーフォーマットへ変換する ETL(Extract Transform Load)で言うTの部分\n",
    "4. 出力したデータをみんなに見やすくするため(BIツールから参照できるように)テーブルを作成する ETL(Extract Transform Load)で言うLの部分\n",
    "\n",
    "よくある、関数の羅列をするのではなく、実業務に沿った形で流れを紹介していきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: hive.exec.dynamic.partition.mode\n",
      "Warning: Ignoring non-Spark config property: hive.exec.dynamic.partition\n",
      "21/10/12 11:00:14 WARN Utils: Your hostname, saitouyuuki-no-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.0.10 instead (on interface en0)\n",
      "21/10/12 11:00:14 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/yuki/Library/Python/3.8/lib/python/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "21/10/12 11:00:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "#pysparkに必要なライブラリを読み込む\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#spark sessionの作成\n",
    "# spark.ui.enabled trueとするとSparkのGUI画面を確認することができます\n",
    "# spark.eventLog.enabled true　とすると　GUIで実行ログを確認することができます\n",
    "# GUIなどの確認は次のチャプターで説明を行います。\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"chapter2\") \\\n",
    "    .config(\"hive.exec.dynamic.partition\", \"true\") \\\n",
    "    .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"JST\") \\\n",
    "    .config(\"spark.ui.enabled\",\"true\") \\\n",
    "    .config(\"spark.eventLog.enabled\",\"true\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データソースの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import LongType, StructType, StructField, StringType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# カラム名、型、デフォルト値で設定していきます\n",
    "struct = StructType([\n",
    "    StructField(\"code\", StringType(), False),\n",
    "    StructField(\"kenmei\", StringType(), False),\n",
    "    StructField(\"gengo\", StringType(), False),\n",
    "    StructField(\"wareki\", StringType(), False),\n",
    "    StructField(\"seireki\", StringType(), False),\n",
    "    StructField(\"chu\", StringType(), False),\n",
    "    StructField(\"sokei\", StringType(), False),\n",
    "    StructField(\"jinko_male\", StringType(), False),\n",
    "    StructField(\"jinko_female\", StringType(), False)\n",
    "])\n",
    "df=spark.read.option(\"multiLine\", \"true\") \\\n",
    "    .option(\"encoding\", \"SJIS\").csv(\"/Users/yuki/pyspark_batch/dataset/jinko.csv\", header=False, sep=',', inferSchema=False,schema=struct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 変換を行う(集計等を行う)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ここでDataFrameの処理と違いSQLの場合は、仮想的なテーブルjinkoを作成します\n",
    "# テーブルを作成することでSQLを発行することができるようになります\n",
    "df.createOrReplaceTempView(\"jinko\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+-----+----------+----------+----+------------+----------+------------+\n",
      "|          code|    kenmei|gengo|    wareki|   seireki| chu|       sokei|jinko_male|jinko_female|\n",
      "+--------------+----------+-----+----------+----------+----+------------+----------+------------+\n",
      "|都道府県コード|都道府県名| 元号|和暦（年）|西暦（年）|  注|人口（総数）|人口（男）|  人口（女）|\n",
      "|            00|      全国| 大正|         9|      1920|null|    55963053|  28044185|    27918868|\n",
      "|            01|    北海道| 大正|         9|      1920|null|     2359183|   1244322|     1114861|\n",
      "|            02|    青森県| 大正|         9|      1920|null|      756454|    381293|      375161|\n",
      "|            03|    岩手県| 大正|         9|      1920|null|      845540|    421069|      424471|\n",
      "|            04|    宮城県| 大正|         9|      1920|null|      961768|    485309|      476459|\n",
      "|            05|    秋田県| 大正|         9|      1920|null|      898537|    453682|      444855|\n",
      "|            06|    山形県| 大正|         9|      1920|null|      968925|    478328|      490597|\n",
      "|            07|    福島県| 大正|         9|      1920|null|     1362750|    673525|      689225|\n",
      "|            08|    茨城県| 大正|         9|      1920|null|     1350400|    662128|      688272|\n",
      "|            09|    栃木県| 大正|         9|      1920|null|     1046479|    514255|      532224|\n",
      "|            10|    群馬県| 大正|         9|      1920|null|     1052610|    514106|      538504|\n",
      "|            11|    埼玉県| 大正|         9|      1920|null|     1319533|    641161|      678372|\n",
      "|            12|    千葉県| 大正|         9|      1920|null|     1336155|    656968|      679187|\n",
      "|            13|    東京都| 大正|         9|      1920|null|     3699428|   1952989|     1746439|\n",
      "|            14|  神奈川県| 大正|         9|      1920|null|     1323390|    689751|      633639|\n",
      "|            15|    新潟県| 大正|         9|      1920|null|     1776474|    871532|      904942|\n",
      "|            16|    富山県| 大正|         9|      1920|null|      724276|    354775|      369501|\n",
      "|            17|    石川県| 大正|         9|      1920|null|      747360|    364375|      382985|\n",
      "|            18|    福井県| 大正|         9|      1920|null|      599155|    293181|      305974|\n",
      "+--------------+----------+-----+----------+----------+----+------------+----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 早速SQLを実行してみます\n",
    "spark.sql(\"select * from jinko\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前のチャプターでDataframeベースで実行していた処理をSparkSQLを使って実装し直してみます。\n",
    "\n",
    "```\n",
    "df_after_t=df.where(df.gengo == \"平成\").groupBy(\"kenmei\") \\\n",
    "    .agg(sf.avg(\"jinko_male\").alias(\"male_avg\"),sf.avg(\"jinko_female\").alias(\"female_avg\")) \\\n",
    "      .filter(df.kenmei != \"人口集中地区以外の地区\").sort(\"male_avg\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+-----------------+\n",
      "|  kenmei|         male_avg|       female_avg|\n",
      "+--------+-----------------+-----------------+\n",
      "|  鳥取県|287885.3333333333|314291.3333333333|\n",
      "|  島根県|         356034.5|388621.6666666667|\n",
      "|  高知県|372268.1666666667|         418517.0|\n",
      "|  徳島県|383399.1666666667|         423152.0|\n",
      "|  福井県|         395512.5|420182.6666666667|\n",
      "|  佐賀県|         408192.5|         456442.5|\n",
      "|  山梨県|425777.8333333333|441831.1666666667|\n",
      "|  香川県|485871.8333333333|523763.6666666667|\n",
      "|和歌山県|         490624.0|547112.3333333334|\n",
      "|  富山県|         532857.0|573049.8333333334|\n",
      "|  宮崎県|         542386.5|         608793.0|\n",
      "|  秋田県|542928.3333333334|         604578.5|\n",
      "|  石川県|         566064.0|         604518.5|\n",
      "|  大分県|571530.6666666666|638773.6666666666|\n",
      "|  山形県|         583603.5|627811.6666666666|\n",
      "|  沖縄県|         654622.0|679050.6666666666|\n",
      "|  岩手県|659592.6666666666|714973.1666666666|\n",
      "|  滋賀県|         662391.0|         680326.0|\n",
      "|  奈良県|671178.6666666666|734736.6666666666|\n",
      "|  青森県|675238.6666666666|751182.1666666666|\n",
      "+--------+-----------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQLで書き直すと、等価な処理は以下になります。\n",
    "\n",
    "df_after_t=spark.sql(\"\"\" \n",
    "\n",
    "select kenmei,avg(jinko_male) as male_avg,avg(jinko_female) as female_avg \n",
    " from jinko\n",
    "  where gengo='平成' and kenmei != '人口集中地区以外の地区'\n",
    " group by kenmei\n",
    " order by male_avg\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "df_after_t.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# テーブル定義はどこに保存されている？\n",
    "\n",
    "1. テンポラリーテーブル（メモリ）\n",
    "2. Create External TABLE（メタデータストア）\n",
    "   1. ローカルの場合metastore_db\n",
    "   2. 本番環境などの場合は、自前で構築する場合はMysql。クラウドサービスであればAmazon Glue Data Catalogなどがあります。\n",
    "\n",
    "今回はローカル環境なのでmetastore_dbと呼ばれるところに格納されています。\n",
    "永続的に保存されるので、前のチャプターで以下のデータベースを作っている人は定義が残っているはずです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#　テーブルの作成\n",
    "spark.sql(\"\"\" \n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS default.jinko_avg ( male_avg double, female_avg double)\n",
    "PARTITIONED BY (kenmei String)\n",
    "STORED AS PARQUET\n",
    "LOCATION '/Users/yuki/pyspark_batch/dataset/parquet/';\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert 文の発行\n",
    "SparkSQLでは Insert文も発行することが可能です  \n",
    "dataframeを吐き出してパーティションを認識させて。。というのは少し面倒  \n",
    "insert 文を使えばその作業を簡略化できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データの登録を行います\n",
    "\n",
    "次に紹介するspark sqlを用いたinsert(とselect)は前セクションで紹介した以下のコード実行と等価です。\n",
    "\n",
    "```\n",
    "df_after_t=df.where(df.gengo == \"平成\").groupBy(\"kenmei\") \\\n",
    "    .agg(sf.avg(\"jinko_male\").alias(\"male_avg\"),sf.avg(\"jinko_female\").alias(\"female_avg\")) \\\n",
    "      .filter(df.kenmei != \"人口集中地区以外の地区\").sort(\"male_avg\")\n",
    "```\n",
    "\n",
    "```\n",
    "df_after_t.repartition(1).write.partitionBy(\"kenmei\").mode(\"overwrite\").parquet(\"/Users/yuki/pyspark_batch/dataset/parquet\")\n",
    "```\n",
    "\n",
    "```\n",
    "spark.sql(\"msck repair table jinko_avg\")\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/12 11:01:02 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "21/10/12 11:01:02 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "21/10/12 11:01:02 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "21/10/12 11:01:02 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "21/10/12 11:01:11 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "21/10/12 11:01:13 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n"
     ]
    }
   ],
   "source": [
    "# パーティションであるkenmeiは一番うしろに記載する。\n",
    "# dynamic partitionと呼ばれる機能で、kenmeiごとにパーティションを振り分けてくれます\n",
    "dataframe=spark.sql(\"\"\" \n",
    "Insert overwrite table jinko_avg PARTITION(kenmei)\n",
    "\n",
    "select avg(jinko_male) as male_avg,avg(jinko_female),kenmei as female_avg \n",
    " from jinko\n",
    "  where gengo='平成' and kenmei != '人口集中地区以外の地区'\n",
    " group by kenmei\n",
    " order by male_avg\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------+\n",
      "|            male_avg|          female_avg|      kenmei|\n",
      "+--------------------+--------------------+------------+\n",
      "|            893167.5|   944959.6666666666|      三重県|\n",
      "|  1268325.3333333333|  1360099.3333333333|      京都府|\n",
      "|4.0840519833333336E7|4.2415789666666664E7|人口集中地区|\n",
      "|            408192.5|            456442.5|      佐賀県|\n",
      "|         6.1816723E7|6.4687834833333336E7|        全国|\n",
      "|           2650310.5|           2861527.0|      兵庫県|\n",
      "|  2665781.3333333335|  2923371.8333333335|      北海道|\n",
      "|           2987847.0|           2974638.5|      千葉県|\n",
      "|            490624.0|   547112.3333333334|    和歌山県|\n",
      "|  3492880.3333333335|  3443447.8333333335|      埼玉県|\n",
      "|   571530.6666666666|   638773.6666666666|      大分県|\n",
      "|   4292675.833333333|           4517115.0|      大阪府|\n",
      "|   671178.6666666666|   734736.6666666666|      奈良県|\n",
      "|           1139561.5|           1191255.0|      宮城県|\n",
      "|            542386.5|            608793.0|      宮崎県|\n",
      "|            532857.0|   573049.8333333334|      富山県|\n",
      "|   709497.8333333334|            791301.5|      山口県|\n",
      "|            583603.5|   627811.6666666666|      山形県|\n",
      "|   425777.8333333333|   441831.1666666667|      山梨県|\n",
      "|  1009389.1666666666|  1073025.1666666667|      岐阜県|\n",
      "+--------------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "spark.sql(\"select * from jinko_avg\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 8\n",
      "-rw-r--r--  1 yuki  staff  741 Oct 12 11:01 part-00025-7388977f-70b8-4baa-8470-a14ee29657b0.c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "# 出来上がったファイル一度みてみましょう(ロケーションは「/Users/yuki/pyspark_batch/dataset/parquet」なのでロケーションはいかにデータが出ているはずです)\n",
    "!ls -l /Users/yuki/pyspark_batch/dataset/parquet/kenmei=三重県"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hint文\n",
    "SQLを実行するときでもスモールファイル問題に対応できるようにrepartitionに相当する機能が存在しています。\n",
    "\n",
    "それがヒント文です。\n",
    "\n",
    "```\n",
    "/** REPARTITION(25) */\n",
    "\n",
    "```\n",
    "\n",
    "Dataframeの操作では以下の部分です。  \n",
    "df_after_t.repartition(1)\n",
    "\n",
    "先程のinsert文は実はそのままでも動くのですが、スモールファイル問題を引き起こす原因にもなってしまいます。  \n",
    "そこでこのヒント文を使って書き直してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# ヒント文をSelectの部分に埋め込無事でファイルのばらつきを抑える(数字は２とすれば、パーティションごとに２つのファイルができるYewYe)\n",
    "dataframe=spark.sql(\"\"\" \n",
    "Insert overwrite table jinko_avg PARTITION(kenmei)\n",
    "\n",
    "select /** REPARTITION(10) */ avg(jinko_male) as male_avg,avg(jinko_female),kenmei as female_avg \n",
    " from jinko\n",
    "  where gengo='平成' and kenmei != '人口集中地区以外の地区'\n",
    " group by kenmei\n",
    " order by male_avg\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 8\n",
      "-rw-r--r--  1 yuki  staff  741 Oct 12 11:14 part-00025-7f783241-3290-4831-b9c3-bdf08ce5d22d.c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "# 出来上がったファイル一度みてみましょう(ロケーションは「/Users/yuki/pyspark_batch/dataset/parquet」なのでロケーションはいかにデータが出ているはずです)\n",
    "!ls -l /Users/yuki/pyspark_batch/dataset/parquet/kenmei=三重県"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# update文やdelete文は？\n",
    "Update/delete文はありません。  \n",
    "ビッグデータの世界では、トランザクションシステムのように対象のレコードだけをピンポイントで更新する機能を有していないものが多いです。  \n",
    "Sparkもその考えからUpdate/delete文を持ち合わせていません。　　\n",
    "\n",
    "何兆というレコードから対象のレコードを探すことはかなり難しいのと、たくさんのデータソースがまじり合うデータ基盤では\n",
    "トランザクションシステムのように、PKを設定することが困難なことが挙げられます"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkSQLを使うメリット\n",
    "\n",
    "- SQLを使った処理はSQLファイルを外部に配置してそのファイルを読み込み実行することで汎用化がしやすい\n",
    "- SQLになれた人であれば操作がしやすい\n",
    "- データエンジニアリングとしては、SparkSQLを使ってシステム構築をするほうが汎用的で容易\n",
    "\n",
    "# DataFrameを使うメリット\n",
    "- データフレームにしかできないような仕事もある。例えば行列の変換などはdataframeのほうが実行しやすいのでデータサイエンスを好む人はDataFrameを使うほうが良い場合もある。\n",
    "\n",
    "## 速度は変わる？\n",
    "速度はどちらでも変わりません。内部的にはSQLはDataFrameとして処理され実行されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark利用の停止\n",
    "spark.stop()\n",
    "spark.sparkContext.stop()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cadefa720d1a2267f4d12d08d812560a64cfe891877bc388bf0e8af3e4846067"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
