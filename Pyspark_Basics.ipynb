{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.0 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "cadefa720d1a2267f4d12d08d812560a64cfe891877bc388bf0e8af3e4846067"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pysparkに必要なライブラリを読み込む\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#spark sessionの作成\n",
    "# spark.ui.enabled trueとするとSparkのGUI画面を確認することができます\n",
    "# spark.eventLog.enabled true　とすると　GUIで実行ログを確認することができます\n",
    "# GUIなどの確認は次のチャプターで説明を行います。\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"chapter2\") \\\n",
    "    .config(\"hive.exec.dynamic.partition\", \"true\") \\\n",
    "    .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"JST\") \\\n",
    "    .config(\"spark.ui.enabled\",\"true\") \\\n",
    "    .config(\"spark.eventLog.enabled\",\"true\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "source": [
    "# Spark Sessionとは？\n",
    "\n",
    "Javaで言うところのインスタンスを作る作業のことです(new Class())。  \n",
    "今回の場合は、アプリケーション名が「chapter2」で作成を行っています。  \n",
    "\n",
    "configの部分で  \n",
    "非常に細かい設定ができるので、詳しくは公式のドキュメントを参考にしてください https://spark.apache.org/docs/3.1.1/  \n",
    "一部メモリの設定いついては「Sparkを本番環境で動かす」チャプターにて紹介します"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Sparkを用いたバッチにおけるデータエンジニアリング一連の流れ\n",
    "\n",
    "1. データソースの読み込み(今回は、人口統計データ(/dataset/jinkou.csv))　ETL(Extract Transform Load)で言うEの部分\n",
    "2. 変換を行う(集計等を行う)　DataFrame処理 or SQL処理の２パターンで実行可能 ETL(Extract Transform Load)で言うTの部分\n",
    "3. カラムナーフォーマットへ変換する ETL(Extract Transform Load)で言うTの部分\n",
    "4. 出力したデータをみんなに見やすくするため(BIツールから参照できるように)テーブルを作成する ETL(Extract Transform Load)で言うLの部分\n",
    "\n",
    "よくある、関数の羅列をするのではなく、実業務に沿った形で流れを紹介していきます。\n",
    "\n",
    "## データソース\n",
    "データの源。リレーショナルデータベースのときもあれば、今回のようにファイルの形式のときもある。  \n",
    "更に進むと、PDFやEXCELなんて事もあります。  \n",
    "ストリーミングだとIoTであったり、Webブラウザのアクセスログだったりとデータになりうるものは無限に存在しています。  \n",
    "\n",
    "## 変換処理\n",
    "ETL（Extract Transform Load）というと少し定義として広いのかもしれないのですが、  \n",
    "データを整形してより分析向けの形(フォーマット変換や圧縮含む)にしたり、精度の高いデータを作成する行為のことです。  \n",
    "そのため、ETLというとバッチ処理のイメージを持つ人も多いかもしれませんが、ストリーミングデータにも適用される言葉です。   \n",
    "\n",
    "Lの処理はSparkにおいてDataFrameもしくはSQLで処理することができる(RDDと呼ばれるものもあるが、労力の割に実際は出番はあまりなく今回は取り扱わない)\n",
    "\n",
    "## カラムナーフォーマットへ変換を行う\n",
    "ビッグデータの世界では、Apache Parquet と呼ばれるフォーマットが広く使われています。  \n",
    "CSV形式のようなローフォーマットはビックデータ処理において処理効率が悪いため、早い段階でParquetに変換を行います。  \n",
    "分析用のSQLの実行であったり、複数台で処理することに向いているフォーマットです。  \n",
    "\n",
    "Parquetの特徴としては以下になります。\n",
    "\n",
    "- カラムナー（ストレージ）フォーマット\n",
    "- カラムごとに圧縮が効くため、効率よくデータをストアできる\n",
    "- 多くのプロダクトがサポートしている\n",
    "\n",
    "多くのプロダクトはParquetを取り込んだり処理したりする機能を提供してくれており単体ではなく総合で使えるフォーマットです。\n",
    "\n",
    "## テーブル形式での保存\n",
    "\n",
    "多くは、実データとテーブル定義が分離された`ロケーション方式`をとっている。  \n",
    "後ほど実際に作成してみますが、イメージは以下のような感じです。\n",
    "\n",
    "```\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS sample.sampletable ( id INT, date STRING)\n",
    "PARTITIONED BY (dt INT)\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "LOCATION '/Users/yuki/pyspark_batch/dataset/parquet/';\n",
    "\n",
    "#S3などであれば、以下のように設定を変えることも可能です。\n",
    "LOCATION 's3://data.platform/sample.db/raw_zone/sampletable/';\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "テーブルとして保存することによって、非エンジニアにも扱いやすくしてデータを提供することが可能です。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# データソースの読み込み"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#データソースの読み込み\n",
    "#sep='\\t'とすればtsvでも読み込みが可能です\n",
    "#multiLineは、CSVやTSVの各カラムに改行が含まれていた時の対策です。\n",
    "df=spark.read.option(\"multiLine\", \"true\").option(\"encoding\", \"SJIS\").csv(\"/Users/yuki/pyspark_batch/dataset/jinko.csv\", header=True, sep=',', inferSchema=False)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "source": [
    "# 変換を行う(集計等を行う)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ここからはETLにおけるTを行っていきます\n",
    "\n",
    "#大正や昭和、平成はもう不要かなと感じたら変換処理にて\n",
    "df_after_t=df.where(df.\"和暦（年）\"== \"令和\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#うーん使いづらい。。(日本語))\n",
    "from pyspark.sql.types import LongType, StructType, StructField, StringType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "#スキーマ設定をしていきましょう\n",
    "# カラム名、型、デフォルト値で設定していきます\n",
    "struct = StructType([\n",
    "    StructField(\"code\", StringType(), False),\n",
    "    StructField(\"kenmei\", StringType(), False),\n",
    "    StructField(\"gengo\", StringType(), False),\n",
    "    StructField(\"wareki\", StringType(), False),\n",
    "    StructField(\"seireki\", StringType(), False),\n",
    "    StructField(\"chu\", StringType(), False),\n",
    "    StructField(\"jinko_male\", StringType(), False),\n",
    "    StructField(\"jinko_female\", StringType(), False)\n",
    "])\n",
    "df=spark.read.option(\"multiLine\", \"true\").option(\"encoding\", \"SJIS\").csv(\"/Users/yuki/pyspark_batch/dataset/jinko.csv\", header=False, sep=',', inferSchema=False,schema=struct)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 大正や昭はもう不要かなと感じたら変換処理にて\n",
    "# ヘッダーも同時に除外してしまう\n",
    "df.where(df.gengo == \"平成\").count()\n",
    "# where 以外にもfilterと呼ばれるものがあります。機能は同じなので好きな方を選んで大丈夫です\n",
    "df.filter(df.gengo == \"平成\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#集計をしてみます\n",
    "#平成の県ごとの男女の数の平均\n",
    "import pyspark.sql.functions as sf\n",
    "df.where(df.gengo == \"平成\").groupBy(\"kenmei\").agg(sf.avg(\"jinko_male\").alias(\"male_avg\"),sf.avg(\"jinko_female\").alias(\"female_avg\")).show()\n",
    "\n",
    "#「人口集中地区以外の地区」がいらなそうですね。\n",
    "# データをクレンジングして不要なデータを除きましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#良さそうです！\n",
    "df.where(df.gengo == \"平成\").groupBy(\"kenmei\").agg(sf.avg(\"jinko_male\").alias(\"male_avg\"),sf.avg(\"jinko_female\").alias(\"female_avg\")).filter(df.kenmei != \"人口集中地区以外の地区\").sort(\"male_avg\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#結果を一度保存しておきます\n",
    "df_after_t=df.where(df.gengo == \"平成\").groupBy(\"kenmei\").agg(sf.avg(\"jinko_male\").alias(\"male_avg\"),sf.avg(\"jinko_female\").alias(\"female_avg\")).filter(df.kenmei != \"人口集中地区以外の地区\").sort(\"male_avg\")"
   ]
  },
  {
   "source": [
    "# DIKWモデル\n",
    "少し脇道にそれるのですが、上記の作業はDIKWモデルというものに沿った動きでです。\n",
    "\n",
    "DIKWモデルでは、データのステージを「Data」「Infromation」「Knowledge」「Wisdom」として定義しています。\n",
    "\n",
    "- Data(データ)\n",
    "- Information（情報）\n",
    "- Knowledge（知識）\n",
    "- Wisdom（知恵）\n",
    "\n",
    "これらの頭文字をとってDIKWモデルと呼ばれています。\n",
    "\n",
    "ETLをすることはDataを情報や知識に変換することを指します。\n",
    "情報や知識は、データから見つかるルールや関係性のことです。\n",
    "今回の場合だと、鳥取県が人口少ないです　という事実がわかったという形になります。\n",
    "\n",
    "知恵は、この知識から生み出すもので、例えば鳥取県の人口が少なく、それが問題なのであれば\n",
    "その問題を解決するための施策が知恵になります。\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# カラムナーフォーマットへ変換する\n",
    "データの変換が終わったので次は、そのデータをビッグデータ向けのフォーマットで保存することを考えます。  \n",
    "今回はParquet形式へデータを変換します。\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#単純に吐き出す方法\n",
    "df_after_t.write.mode(\"overwrite\").parquet(\"/Users/yuki/pyspark_batch/dataset/parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "total 24\ndrwxr-xr-x  6 yuki  staff   192 Sep  8 17:58 \u001b[34m.\u001b[m\u001b[m\ndrwxr-xr-x  4 yuki  staff   128 Sep  8 17:58 \u001b[34m..\u001b[m\u001b[m\n-rw-r--r--  1 yuki  staff     8 Sep  8 17:58 ._SUCCESS.crc\n-rw-r--r--  1 yuki  staff    24 Sep  8 17:58 .part-00000-3be5d55f-9d08-4db4-b259-258609a9c292-c000.snappy.parquet.crc\n-rw-r--r--  1 yuki  staff     0 Sep  8 17:58 _SUCCESS\n-rw-r--r--  1 yuki  staff  2004 Sep  8 17:58 part-00000-3be5d55f-9d08-4db4-b259-258609a9c292-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "#ファイルを見てみます\n",
    "!ls -al /Users/yuki/pyspark_batch/dataset/parquet\n",
    "\n",
    "#　ファイルが多いですね。。 一つのファイルサイズが小さいのも気になります"
   ]
  },
  {
   "source": [
    "#　スモールファイル問題\n",
    "オンプレでもクラウドでもそうなのですが、ビッグデータの世界では一つのファイルが小さすぎると途端に処理が遅くなります。  \n",
    "この問題をスモールファイル問題と読んでいます。  \n",
    "一般に、１GBくらいずつまとめることが推奨されています。\n",
    "\n",
    "この問題を解決するためには、repartition(もしくはcolaese)を使ってファイルをマージする必要があります。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 今回はファイルを一個に纏めてみようと思います。\n",
    "df_after_t.repartition(1).write.mode(\"overwrite\").parquet(\"/Users/yuki/pyspark_batch/dataset/parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "total 8\n-rw-r--r--  1 yuki  staff     0 Sep  8 17:58 _SUCCESS\n-rw-r--r--  1 yuki  staff  2004 Sep  8 17:58 part-00000-3be5d55f-9d08-4db4-b259-258609a9c292-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "# もう一度ファイルを見てみます\n",
    "!ls -l /Users/yuki/pyspark_batch/dataset/parquet\n",
    "# 一つになりました！ スモールファイル問題も解決です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    }
   ],
   "source": [
    "#もう少し書き込みのオプションを見ていきます\n",
    "# partitionByを使うことで、データをパーティションごと(次に説明します)に分けて配置することができます。\n",
    "# 今回はkenmei(県名)ごとにデータを保存してみようと思います。\n",
    "df_after_t.repartition(1).write.partitionBy(\"kenmei\").mode(\"overwrite\").parquet(\"/Users/yuki/pyspark_batch/dataset/parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "total 0\n-rw-r--r--  1 yuki  staff    0 Sep  8 20:36 _SUCCESS\ndrwxr-xr-x  4 yuki  staff  128 Sep  8 20:36 \u001b[34mkenmei=三重県\u001b[m\u001b[m\ndrwxr-xr-x  4 yuki  staff  128 Sep  8 20:36 \u001b[34mkenmei=京都府\u001b[m\u001b[m\ndrwxr-xr-x  4 yuki  staff  128 Sep  8 20:36 \u001b[34mkenmei=人口集中地区\u001b[m\u001b[m\ndrwxr-xr-x  4 yuki  staff  128 Sep  8 20:36 \u001b[34mkenmei=佐賀県\u001b[m\u001b[m\ndrwxr-xr-x  4 yuki  staff  128 Sep  8 20:36 \u001b[34mkenmei=全国\u001b[m\u001b[m\ndrwxr-xr-x  4 yuki  staff  128 Sep  8 20:36 \u001b[34mkenmei=兵庫県\u001b[m\u001b[m\ndrwxr-xr-x  4 yuki  staff  128 Sep  8 20:36 \u001b[34mkenmei=北海道\u001b[m\u001b[m\ndrwxr-xr-x  4 yuki  staff  128 Sep  8 20:36 \u001b[34mkenmei=千葉県\u001b[m\u001b[m\ndrwxr-xr-x  4 yuki  staff  128 Sep  8 20:36 \u001b[34mkenmei=和歌山県\u001b[m\u001b[m\ndrwxr-xr-x  4 yuki  staff  128 Sep  8 20:36 \u001b[34mkenmei=埼玉県\u001b[m\u001b[m\ndrwxr-xr-x  4 yuki  staff  128 Sep  8 20:36 \u001b[34mkenmei=大分県\u001b[m\u001b[m\ndrwxr-xr-x  4 yuki  staff  128 Sep  8 20:36 \u001b[34mkenmei=大阪府\u001b[m\u001b[m\ndrwxr-xr-x  4 yuki  staff  128 Sep  8 20:36 \u001b[34mkenmei=奈良県\u001b[m\u001b[m\ndrwxr-xr-x  4 yuki  staff  128 Sep  8 20:36 \u001b[34mkenmei=宮城県\u001b[m\u001b[m\ndrwxr-xr-x  4 yuki  staff  128 Sep  8 20:36 \u001b[34mkenmei=宮崎県\u001b[m\u001b[m\ndrwxr-xr-x  4 yuki  staff  128 Sep  8 20:36 \u001b[34mkenmei=富山県\u001b[m\u001b[m\ndrwxr-xr-x  4 yuki  staff  128 Sep  8 20:36 \u001b[34mkenmei=山口県\u001b[m\u001b[m\ndrwxr-xr-x  4 yuki  staff  128 Sep  8 20:36 \u001b[34mkenmei=山形県\u001b[m\u001b[m\ndrwxr-xr-x  4 yuki  staff  128 Sep  8 20:36 \u001b[34mkenmei=山梨県\u001b[m\u001b[m\ndrwxr-xr-x  4 yuki  staff  128 Sep  8 20:36 \u001b[34mkenmei=岐阜県\u001b[m\u001b[m\ndrwxr-xr-x  4 yuki  staff  128 Sep  8 20:36 \u001b[34mkenmei=岡山県\u001b[m\u001b[m\ndrwxr-xr-x  4 yuki  staff  128 Sep  8 20:36 \u001b[34mkenmei=岩手県\u001b[m\u001b[m\ndrwxr-xr-x  4 yuki  staff  128 Sep  8 20:36 \u001b[34mkenmei=島根県\u001b[m\u001b[m\ndrwxr-xr-x  4 yuki  staff  128 Sep  8 20:36 \u001b[34mkenmei=広島県\u001b[m\u001b[m\ndrwxr-xr-x  4 yuki  staff  128 Sep  8 20:36 \u001b[34mkenmei=徳島県\u001b[m\u001b[m\ndrwxr-xr-x  4 yuki  staff  128 Sep  8 20:36 \u001b[34mkenmei=愛媛県\u001b[m\u001b[m\ndrwxr-xr-x  4 yuki  staff  128 Sep  8 20:36 \u001b[34mkenmei=愛知県\u001b[m\u001b[m\ndrwxr-xr-x  4 yuki  staff  128 Sep  8 20:36 \u001b[34mkenmei=新潟県\u001b[m\u001b[m\ndrwxr-xr-x  4 yuki  staff  128 Sep  8 20:36 \u001b[34mkenmei=東京都\u001b[m\u001b[m\ndrwxr-xr-x  4 yuki  staff  128 Sep  8 20:36 \u001b[34mkenmei=栃木県\u001b[m\u001b[m\ndrwxr-xr-x  4 yuki  staff  128 Sep  8 20:36 \u001b[34mkenmei=沖縄県\u001b[m\u001b[m\ndrwxr-xr-x  4 yuki  staff  128 Sep  8 20:36 \u001b[34mkenmei=滋賀県\u001b[m\u001b[m\ndrwxr-xr-x  4 yuki  staff  128 Sep  8 20:36 \u001b[34mkenmei=熊本県\u001b[m\u001b[m\ndrwxr-xr-x  4 yuki  staff  128 Sep  8 20:36 \u001b[34mkenmei=石川県\u001b[m\u001b[m\ndrwxr-xr-x  4 yuki  staff  128 Sep  8 20:36 \u001b[34mkenmei=神奈川県\u001b[m\u001b[m\ndrwxr-xr-x  4 yuki  staff  128 Sep  8 20:36 \u001b[34mkenmei=福井県\u001b[m\u001b[m\ndrwxr-xr-x  4 yuki  staff  128 Sep  8 20:36 \u001b[34mkenmei=福岡県\u001b[m\u001b[m\ndrwxr-xr-x  4 yuki  staff  128 Sep  8 20:36 \u001b[34mkenmei=福島県\u001b[m\u001b[m\ndrwxr-xr-x  4 yuki  staff  128 Sep  8 20:36 \u001b[34mkenmei=秋田県\u001b[m\u001b[m\ndrwxr-xr-x  4 yuki  staff  128 Sep  8 20:36 \u001b[34mkenmei=群馬県\u001b[m\u001b[m\ndrwxr-xr-x  4 yuki  staff  128 Sep  8 20:36 \u001b[34mkenmei=茨城県\u001b[m\u001b[m\ndrwxr-xr-x  4 yuki  staff  128 Sep  8 20:36 \u001b[34mkenmei=長崎県\u001b[m\u001b[m\ndrwxr-xr-x  4 yuki  staff  128 Sep  8 20:36 \u001b[34mkenmei=長野県\u001b[m\u001b[m\ndrwxr-xr-x  4 yuki  staff  128 Sep  8 20:36 \u001b[34mkenmei=青森県\u001b[m\u001b[m\ndrwxr-xr-x  4 yuki  staff  128 Sep  8 20:36 \u001b[34mkenmei=静岡県\u001b[m\u001b[m\ndrwxr-xr-x  4 yuki  staff  128 Sep  8 20:36 \u001b[34mkenmei=香川県\u001b[m\u001b[m\ndrwxr-xr-x  4 yuki  staff  128 Sep  8 20:36 \u001b[34mkenmei=高知県\u001b[m\u001b[m\ndrwxr-xr-x  4 yuki  staff  128 Sep  8 20:36 \u001b[34mkenmei=鳥取県\u001b[m\u001b[m\ndrwxr-xr-x  4 yuki  staff  128 Sep  8 20:36 \u001b[34mkenmei=鹿児島県\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "# もう一度ファイルを見てみます\n",
    "!ls -l /Users/yuki/pyspark_batch/dataset/parquet\n",
    "\n",
    "#県名が出てきました\n",
    "三重県のデータはkenmei=三重県の下に格納されています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "total 8\n-rw-r--r--  1 yuki  staff  741 Sep  8 20:36 part-00000-edc190f6-8222-4c3a-afa5-b1f74d651935.c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "#　三重県のデータを見てみます\n",
    "!ls -l /Users/yuki/pyspark_batch/dataset/parquet/kenmei=三重県"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------------------+----------+\n|          male_avg|female_avg|\n+------------------+----------+\n|1838127.1666666667|  893167.5|\n+------------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# ここでSparkでParquetのデータを読み込んでみます\n",
    "\n",
    "parquet_df=spark.read.parquet(\"/Users/yuki/pyspark_batch/dataset/parquet/kenmei=三重県\")\n",
    "parquet_df.show()\n",
    "\n",
    "#コレが三重県の男性と女性の人口の平均値です。"
   ]
  },
  {
   "source": [
    "# 出力したデータをみんなに見やすくするため(BIツールから参照できるように)テーブルを作成する\n",
    "次はテーブルの作成を行ってみようと思います。  \n",
    "今のままだとエンジニア向けでちょっと使い勝手が悪いのとBIツールといった他のツールから参照することができません"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "21/09/08 20:50:17 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "21/09/08 20:50:17 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "21/09/08 20:50:21 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "21/09/08 20:50:21 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore yuki@127.0.0.1\n",
      "21/09/08 20:50:21 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException\n",
      "21/09/08 20:50:22 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "21/09/08 20:50:22 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "21/09/08 20:50:22 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "21/09/08 20:50:22 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "metadata": {},
     "execution_count": 145
    }
   ],
   "source": [
    "# Createテーブルを発行します\n",
    "# 次のチャプターでも紹介しますが、spark.sqlという関数を使います\n",
    "\n",
    "#jinko_avgテーブルを作成します。\n",
    "#パーティションとはデータを分けるフォルダみたいなもの、パーティションを分けることで読み込むデータ量を少なくしたりできるので最適化できる\n",
    "#先程確認したkenmei=の部分がパーティションになっているのでその出力結果に合わせてテーブルを作成してみます。\n",
    "\n",
    "#ロケーションは、kenmeiを含まずに指定します。\n",
    "\n",
    "spark.sql(\"\"\" \n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS default.jinko_avg ( male_avg double, female_avg double)\n",
    "PARTITIONED BY (kenmei String)\n",
    "STORED AS PARQUET\n",
    "LOCATION '/Users/yuki/pyspark_batch/dataset/parquet/';\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------+---------+-----------+\n|database|tableName|isTemporary|\n+--------+---------+-----------+\n| default|jinko_avg|      false|\n+--------+---------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# テーブルを見てみます。\n",
    "spark.sql(\"show tables\").show()\n",
    "\n",
    "# ちゃんとできているようですね。\n",
    "# ちなみに実行しているSQLは実はSQLみたいなものでHiveSQLと呼ばれるものです。\n",
    "# Mysqlの扱いとほとんど同じなので、Mysqlみたいに使って動かなかったところだけ検索すると効率が良いと思います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------+----------+------+\n|male_avg|female_avg|kenmei|\n+--------+----------+------+\n+--------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "#　おや。データを見ることができません。。\n",
    "spark.sql(\"select * from default.jinko_avg\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "metadata": {},
     "execution_count": 154
    }
   ],
   "source": [
    "# テーブルだけでなく、Partitionを認識させてあげないといけません\n",
    "# msck repair table　テーブル名と実行するとパーティションが認識されます(ちなみにAdd partitionというコマンドもあります)。\n",
    "spark.sql(\"msck repair table jinko_avg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------------+--------------------+------------+\n|            male_avg|          female_avg|      kenmei|\n+--------------------+--------------------+------------+\n|  1838127.1666666667|            893167.5|      三重県|\n|  2628424.6666666665|  1268325.3333333333|      京都府|\n|        8.32563095E7|4.0840519833333336E7|人口集中地区|\n|            864635.0|            408192.5|      佐賀県|\n|1.2650455783333333E8|         6.1816723E7|        全国|\n|           5511837.5|           2650310.5|      兵庫県|\n|   5589153.166666667|  2665781.3333333335|      北海道|\n|           5962485.5|           2987847.0|      千葉県|\n|  1037736.3333333334|            490624.0|    和歌山県|\n|   6936328.166666667|  3492880.3333333335|      埼玉県|\n|  1210304.3333333333|   571530.6666666666|      大分県|\n|   8809790.833333334|   4292675.833333333|      大阪府|\n|  1405915.3333333333|   671178.6666666666|      奈良県|\n|           2330816.5|           1139561.5|      宮城県|\n|           1151179.5|            542386.5|      宮崎県|\n|  1105906.8333333333|            532857.0|      富山県|\n|  1500799.3333333333|   709497.8333333334|      山口県|\n|  1211415.1666666667|            583603.5|      山形県|\n|            867609.0|   425777.8333333333|      山梨県|\n|  2082414.3333333333|  1009389.1666666666|      岐阜県|\n+--------------------+--------------------+------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# 今一度検索をしてみます。\n",
    "spark.sql(\"select * from default.jinko_avg\").show()\n",
    "#　今度は出ましたね！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------------+-----------------+------+\n|            male_avg|       female_avg|kenmei|\n+--------------------+-----------------+------+\n|1.2490754833333334E7|6222455.666666667|東京都|\n+--------------------+-----------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# もちろんSQLなので whereも可能です\n",
    "spark.sql(\"select * from default.jinko_avg where kenmei='東京都'\").show()"
   ]
  },
  {
   "source": [
    "# 次のチャプターはSparkSQLについて紹介していこうと思います"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark利用の停止\n",
    "spark.stop()\n",
    "spark.sparkContext.stop()"
   ]
  }
 ]
}