{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.0 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "cadefa720d1a2267f4d12d08d812560a64cfe891877bc388bf0e8af3e4846067"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pysparkに必要なライブラリを読み込む\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#spark sessionの作成\n",
    "# spark.ui.enabled trueとするとSparkのGUI画面を確認することができます\n",
    "# spark.eventLog.enabled true　とすると　GUIで実行ログを確認することができます\n",
    "# GUIなどの確認は次のチャプターで説明を行います。\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"chapter2\") \\\n",
    "    .config(\"hive.exec.dynamic.partition\", \"true\") \\\n",
    "    .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"JST\") \\\n",
    "    .config(\"spark.ui.enabled\",\"true\") \\\n",
    "    .config(\"spark.eventLog.enabled\",\"true\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "source": [
    "# Spark Sessionとは？\n",
    "\n",
    "Javaで言うところのインスタンスを作る作業のことです(new Class())。  \n",
    "今回の場合は、アプリケーション名が「chapter2」で作成を行っています。  \n",
    "\n",
    "configの部分で  \n",
    "非常に細かい設定ができるので、詳しくは公式のドキュメントを参考にしてください https://spark.apache.org/docs/3.1.1/  \n",
    "一部メモリの設定いついては「Sparkを本番環境で動かす」チャプターにて紹介します"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Sparkを用いたバッチにおけるデータエンジニアリング一連の流れ\n",
    "\n",
    "1. データソースの読み込み(今回は、人口統計データ(/dataset/jinkou.csv))　ETL(Extract Transform Load)で言うEの部分\n",
    "2. 変換を行う(集計等を行う)　DataFrame処理 or SQL処理の２パターンで実行可能 ETL(Extract Transform Load)で言うTの部分\n",
    "3. カラムナーフォーマットへ変換する ETL(Extract Transform Load)で言うTの部分\n",
    "4. 出力したデータをみんなに見やすくするため(BIツールから参照できるように)テーブルを作成する ETL(Extract Transform Load)で言うLの部分\n",
    "\n",
    "よくある、関数の羅列をするのではなく、実業務に沿った形で流れを紹介していきます。\n",
    "\n",
    "## データソース\n",
    "データの源。リレーショナルデータベースのときもあれば、今回のようにファイルの形式のときもある。  \n",
    "更に進むと、PDFやEXCELなんて事もあります。  \n",
    "ストリーミングだとIoTであったり、Webブラウザのアクセスログだったりとデータになりうるものは無限に存在しています。  \n",
    "\n",
    "## 変換処理\n",
    "ETL（Extract Transform Load）というと少し定義として広いのかもしれないのですが、  \n",
    "データを整形してより分析向けの形(フォーマット変換や圧縮含む)にしたり、精度の高いデータを作成する行為のことです。  \n",
    "そのため、ETLというとバッチ処理のイメージを持つ人も多いかもしれませんが、ストリーミングデータにも適用される言葉です。   \n",
    "\n",
    "Lの処理はSparkにおいてDataFrameもしくはSQLで処理することができる(RDDと呼ばれるものもあるが、労力の割に実際は出番はあまりなく今回は取り扱わない)\n",
    "\n",
    "## カラムナーフォーマットへ変換を行う\n",
    "ビッグデータの世界では、Apache Parquet と呼ばれるフォーマットが広く使われています。  \n",
    "CSV形式のようなローフォーマットはビックデータ処理において処理効率が悪いため、早い段階でParquetに変換を行います。  \n",
    "分析用のSQLの実行であったり、複数台で処理することに向いているフォーマットです。  \n",
    "\n",
    "Parquetの特徴としては以下になります。\n",
    "\n",
    "- カラムナー（ストレージ）フォーマット\n",
    "- カラムごとに圧縮が効くため、効率よくデータをストアできる\n",
    "- 多くのプロダクトがサポートしている\n",
    "\n",
    "多くのプロダクトはParquetを取り込んだり処理したりする機能を提供してくれており単体ではなく総合で使えるフォーマットです。\n",
    "\n",
    "## テーブル形式での保存\n",
    "\n",
    "多くは、実データとテーブル定義が分離された`ロケーション方式`をとっている。  \n",
    "後ほど実際に作成してみますが、イメージは以下のような感じです。\n",
    "\n",
    "```\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS sample.sampletable ( id INT, date STRING)\n",
    "PARTITIONED BY (dt INT)\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "LOCATION '/Users/yuki/pyspark_batch/dataset/parquet/';\n",
    "\n",
    "#S3などであれば、以下のように設定を変えることも可能です。\n",
    "LOCATION 's3://data.platform/sample.db/raw_zone/sampletable/';\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "テーブルとして保存することによって、非エンジニアにも扱いやすくしてデータを提供することが可能です。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# データソースの読み込み"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#データソースの読み込み\n",
    "#sep='\\t'とすればtsvでも読み込みが可能です\n",
    "#multiLineは、CSVやTSVの各カラムに改行が含まれていた時の対策です。\n",
    "df=spark.read.option(\"multiLine\", \"true\").option(\"encoding\", \"SJIS\").csv(\"/Users/yuki/pyspark_batch/dataset/jinko.csv\", header=True, sep=',', inferSchema=False)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "source": [
    "# 変換を行う(集計等を行う)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ここからはETLにおけるTを行っていきます\n",
    "\n",
    "#大正や昭和、平成はもう不要かなと感じたら変換処理にて\n",
    "df_after_t=df.where(df.\"和暦（年）\"== \"令和\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#うーん使いづらい。。(日本語))\n",
    "from pyspark.sql.types import LongType, StructType, StructField, StringType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "#スキーマ設定をしていきましょう\n",
    "# カラム名、型、デフォルト値で設定していきます\n",
    "struct = StructType([\n",
    "    StructField(\"code\", StringType(), False),\n",
    "    StructField(\"kenmei\", StringType(), False),\n",
    "    StructField(\"gengo\", StringType(), False),\n",
    "    StructField(\"wareki\", StringType(), False),\n",
    "    StructField(\"seireki\", StringType(), False),\n",
    "    StructField(\"chu\", StringType(), False),\n",
    "    StructField(\"jinko_male\", StringType(), False),\n",
    "    StructField(\"jinko_female\", StringType(), False)\n",
    "])\n",
    "df=spark.read.option(\"multiLine\", \"true\").option(\"encoding\", \"SJIS\").csv(\"/Users/yuki/pyspark_batch/dataset/jinko.csv\", header=False, sep=',', inferSchema=False,schema=struct)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 大正や昭はもう不要かなと感じたら変換処理にて\n",
    "# ヘッダーも同時に除外してしまう\n",
    "df.where(df.gengo == \"平成\").count()\n",
    "# where 以外にもfilterと呼ばれるものがあります。機能は同じなので好きな方を選んで大丈夫です\n",
    "df.filter(df.gengo == \"平成\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#集計をしてみます\n",
    "#平成の県ごとの男女の数の平均\n",
    "import pyspark.sql.functions as sf\n",
    "df.where(df.gengo == \"平成\").groupBy(\"kenmei\").agg(sf.avg(\"jinko_male\").alias(\"male_avg\"),sf.avg(\"jinko_female\").alias(\"female_avg\")).show()\n",
    "\n",
    "#「人口集中地区以外の地区」がいらなそうですね。\n",
    "# データをクレンジングして不要なデータを除きましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#良さそうです！\n",
    "df.where(df.gengo == \"平成\").groupBy(\"kenmei\").agg(sf.avg(\"jinko_male\").alias(\"male_avg\"),sf.avg(\"jinko_female\").alias(\"female_avg\")).filter(df.kenmei != \"人口集中地区以外の地区\").sort(\"male_avg\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#結果を一度保存しておきます\n",
    "df_after_t=df.where(df.gengo == \"平成\").groupBy(\"kenmei\").agg(sf.avg(\"jinko_male\").alias(\"male_avg\"),sf.avg(\"jinko_female\").alias(\"female_avg\")).filter(df.kenmei != \"人口集中地区以外の地区\").sort(\"male_avg\")"
   ]
  },
  {
   "source": [
    "# DIKWモデル\n",
    "少し脇道にそれるのですが、上記の作業はDIKWモデルというものに沿った動きでです。\n",
    "\n",
    "DIKWモデルでは、データのステージを「Data」「Infromation」「Knowledge」「Wisdom」として定義しています。\n",
    "\n",
    "- Data(データ)\n",
    "- Information（情報）\n",
    "- Knowledge（知識）\n",
    "- Wisdom（知恵）\n",
    "\n",
    "これらの頭文字をとってDIKWモデルと呼ばれています。\n",
    "\n",
    "ETLをすることはDataを情報や知識に変換することを指します。\n",
    "情報や知識は、データから見つかるルールや関係性のことです。\n",
    "今回の場合だと、鳥取県が人口少ないです　という事実がわかったという形になります。\n",
    "\n",
    "知恵は、この知識から生み出すもので、例えば鳥取県の人口が少なく、それが問題なのであれば\n",
    "その問題を解決するための施策が知恵になります。\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# カラムナーフォーマットへ変換する\n",
    "データの変換が終わったので次は、そのデータをビッグデータ向けのフォーマットで保存することを考えます。  \n",
    "今回はParquet形式へデータを変換します。\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#単純に吐き出す方法\n",
    "df_after_t.write.mode(\"overwrite\").parquet(\"/Users/yuki/pyspark_batch/dataset/parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ファイルを見てみます\n",
    "!ls -al /Users/yuki/pyspark_batch/dataset/parquet\n",
    "\n",
    "#　ファイルが多いですね。。 一つのファイルサイズが小さいのも気になります"
   ]
  },
  {
   "source": [
    "#　スモールファイル問題\n",
    "オンプレでもクラウドでもそうなのですが、ビッグデータの世界では一つのファイルが小さすぎると途端に処理が遅くなります。  \n",
    "この問題をスモールファイル問題と読んでいます。  \n",
    "一般に、１GBくらいずつまとめることが推奨されています。\n",
    "\n",
    "この問題を解決するためには、repartition(もしくはcolaese)を使ってファイルをマージする必要があります。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 今回はファイルを一個に纏めてみようと思います。\n",
    "df_after_t.repartition(1).write.mode(\"overwrite\").parquet(\"/Users/yuki/pyspark_batch/dataset/parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# もう一度ファイルを見てみます\n",
    "!ls -l /Users/yuki/pyspark_batch/dataset/parquet\n",
    "# 一つになりました！ スモールファイル問題も解決です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#もう少し書き込みのオプションを見ていきます\n",
    "# partitionByを使うことで、データをパーティションごと(次に説明します)に分けて配置することができます。\n",
    "# 今回はkenmei(県名)ごとにデータを保存してみようと思います。\n",
    "df_after_t.repartition(1).write.partitionBy(\"kenmei\").mode(\"overwrite\").parquet(\"/Users/yuki/pyspark_batch/dataset/parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# もう一度ファイルを見てみます\n",
    "!ls -l /Users/yuki/pyspark_batch/dataset/parquet\n",
    "\n",
    "#県名が出てきました\n",
    "三重県のデータはkenmei=三重県の下に格納されています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#　三重県のデータを見てみます\n",
    "!ls -l /Users/yuki/pyspark_batch/dataset/parquet/kenmei=三重県"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ここでSparkでParquetのデータを読み込んでみます\n",
    "\n",
    "parquet_df=spark.read.parquet(\"/Users/yuki/pyspark_batch/dataset/parquet/kenmei=三重県\")\n",
    "parquet_df.show()\n",
    "\n",
    "#コレが三重県の男性と女性の人口の平均値です。"
   ]
  },
  {
   "source": [
    "# 出力したデータをみんなに見やすくするため(BIツールから参照できるように)テーブルを作成する\n",
    "次はテーブルの作成を行ってみようと思います。  \n",
    "今のままだとエンジニア向けでちょっと使い勝手が悪いのとBIツールといった他のツールから参照することができません"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Createテーブルを発行します\n",
    "# 次のチャプターでも紹介しますが、spark.sqlという関数を使います\n",
    "\n",
    "#jinko_avgテーブルを作成します。\n",
    "#パーティションとはデータを分けるフォルダみたいなもの、パーティションを分けることで読み込むデータ量を少なくしたりできるので最適化できる\n",
    "#先程確認したkenmei=の部分がパーティションになっているのでその出力結果に合わせてテーブルを作成してみます。\n",
    "\n",
    "#ロケーションは、kenmeiを含まずに指定します。\n",
    "\n",
    "spark.sql(\"\"\" \n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS default.jinko_avg ( male_avg double, female_avg double)\n",
    "PARTITIONED BY (kenmei String)\n",
    "STORED AS PARQUET\n",
    "LOCATION '/Users/yuki/pyspark_batch/dataset/parquet/';\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テーブルを見てみます。\n",
    "spark.sql(\"show tables\").show()\n",
    "\n",
    "# ちゃんとできているようですね。\n",
    "# ちなみに実行しているSQLは実はSQLみたいなものでHiveSQLと呼ばれるものです。\n",
    "# Mysqlの扱いとほとんど同じなので、Mysqlみたいに使って動かなかったところだけ検索すると効率が良いと思います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#　おや。データを見ることができません。。\n",
    "spark.sql(\"select * from default.jinko_avg\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テーブルだけでなく、Partitionを認識させてあげないといけません\n",
    "# msck repair table　テーブル名と実行するとパーティションが認識されます(ちなみにAdd partitionというコマンドもあります)。\n",
    "spark.sql(\"msck repair table jinko_avg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 今一度検索をしてみます。\n",
    "spark.sql(\"select * from default.jinko_avg\").show()\n",
    "#　今度は出ましたね！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# もちろんSQLなので whereも可能です\n",
    "spark.sql(\"select * from default.jinko_avg where kenmei='東京都'\").show()"
   ]
  },
  {
   "source": [
    "# 次のチャプターはSparkSQLについて紹介していこうと思います"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark利用の停止\n",
    "spark.stop()\n",
    "spark.sparkContext.stop()"
   ]
  }
 ]
}