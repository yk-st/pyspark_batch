{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.0 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "cadefa720d1a2267f4d12d08d812560a64cfe891877bc388bf0e8af3e4846067"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pysparkに必要なライブラリを読み込む\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#spark sessionの作成\n",
    "# spark.ui.enabled trueとするとSparkのGUI画面を確認することができます\n",
    "# spark.eventLog.enabled true　とすると　GUIで実行ログを確認することができます\n",
    "# GUIなどの確認は次のチャプターで説明を行います。\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"chapter2\") \\\n",
    "    .config(\"hive.exec.dynamic.partition\", \"true\") \\\n",
    "    .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"JST\") \\\n",
    "    .config(\"spark.ui.enabled\",\"true\") \\\n",
    "    .config(\"spark.eventLog.enabled\",\"true\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "source": [
    "# Spark Sessionとは？\n",
    "\n",
    "Javaで言うところのインスタンスを作る作業のことです(new Class())。  \n",
    "今回の場合は、アプリケーション名が「chapter2」で作成を行っています。  \n",
    "\n",
    "configの部分で  \n",
    "非常に細かい設定ができるので、詳しくは公式のドキュメントを参考にしてください https://spark.apache.org/docs/3.1.1/  \n",
    "一部メモリの設定いついては「Sparkを本番環境で動かす」チャプターにて紹介します"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Sparkを用いたバッチにおけるデータエンジニアリング一連の流れ\n",
    "\n",
    "1. データソースの読み込み(今回は、人口統計データ(/dataset/jinkou.csv))　ETL(Extract Transform Load)で言うEの部分\n",
    "2. 変換を行う(集計等を行う)　DataFrame処理 or SQL処理の２パターンで実行可能 ETL(Extract Transform Load)で言うTの部分\n",
    "3. カラムナーフォーマットへ変換する ETL(Extract Transform Load)で言うTの部分\n",
    "4. 出力したデータをみんなに見やすくするため(BIツールから参照できるように)テーブルを作成する ETL(Extract Transform Load)で言うLの部分\n",
    "\n",
    "よくある、関数の羅列をするのではなく、実業務に沿った形で流れを紹介していきます。\n",
    "\n",
    "## データソース\n",
    "データの源。リレーショナルデータベースのときもあれば、今回のようにファイルの形式のときもある。  \n",
    "更に進むと、PDFやEXCELなんて事もあります。  \n",
    "ストリーミングだとIoTであったり、Webブラウザのアクセスログだったりとデータになりうるものは無限に存在しています。  \n",
    "\n",
    "## 変換処理\n",
    "ETL（Extract Transform Load）というと少し定義として広いのかもしれないのですが、  \n",
    "データを整形してより分析向けの形(フォーマット変換や圧縮含む)にしたり、精度の高いデータを作成する行為のことです。  \n",
    "そのため、ETLというとバッチ処理のイメージを持つ人も多いかもしれませんが、ストリーミングデータにも適用される言葉です。   \n",
    "\n",
    "Lの処理はSparkにおいてDataFrameもしくはSQLで処理することができる(RDDと呼ばれるものもあるが、労力の割に実際は出番はあまりなく今回は取り扱わない)\n",
    "\n",
    "## カラムナーフォーマットへ変換を行う\n",
    "ビッグデータの世界では、Apache Parquet と呼ばれるフォーマットが広く使われています。  \n",
    "CSV形式のようなローフォーマットはビックデータ処理において処理効率が悪いため、早い段階でParquetに変換を行います。  \n",
    "分析用のSQLの実行であったり、複数台で処理することに向いているフォーマットです。  \n",
    "\n",
    "Parquetの特徴としては以下になります。\n",
    "\n",
    "- カラムナー（ストレージ）フォーマット\n",
    "- カラムごとに圧縮が効くため、効率よくデータをストアできる\n",
    "- 多くのプロダクトがサポートしている\n",
    "\n",
    "多くのプロダクトはParquetを取り込んだり処理したりする機能を提供してくれており単体ではなく総合で使えるフォーマットです。\n",
    "\n",
    "## テーブル形式での保存\n",
    "\n",
    "多くは、実データとテーブル定義が分離された`ロケーション方式`をとっている。  \n",
    "後ほど実際に作成してみますが、イメージは以下のような感じです。\n",
    "\n",
    "```\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS sample.sampletable ( id INT, date STRING)\n",
    "PARTITIONED BY (dt INT)\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "LOCATION '/Users/yuki/pyspark_batch/dataset/parquet/';\n",
    "\n",
    "#S3などであれば、以下のように設定を変えることも可能です。\n",
    "LOCATION 's3://data.platform/sample.db/raw_zone/sampletable/';\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "テーブルとして保存することによって、非エンジニアにも扱いやすくしてデータを提供することが可能です。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#データソースの読み込み\n",
    "#sep='\\t'とすればtsvでも読み込みが可能です\n",
    "#multiLineは、CSVやTSVの各カラムに改行が含まれていた時の対策です。\n",
    "df=spark.read.option(\"multiLine\", \"true\").option(\"encoding\", \"SJIS\").csv(\"/Users/yuki/pyspark_batch/dataset/jinko.csv\", header=True, sep=',', inferSchema=False)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#大正や昭和、平成はもう不要かなと感じたら変換処理にて\n",
    "df_after_t=df.where(df.\"和暦（年）\"== \"令和\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------+----------+-----+----------+----------+----+------------+------------+\n|          code|    kenmei|gengo|    wareki|   seireki| chu|  jinko_male|jinko_female|\n+--------------+----------+-----+----------+----------+----+------------+------------+\n|都道府県コード|都道府県名| 元号|和暦（年）|西暦（年）|  注|人口（総数）|  人口（男）|\n|            00|      全国| 大正|         9|      1920|null|    55963053|    28044185|\n|            01|    北海道| 大正|         9|      1920|null|     2359183|     1244322|\n|            02|    青森県| 大正|         9|      1920|null|      756454|      381293|\n|            03|    岩手県| 大正|         9|      1920|null|      845540|      421069|\n|            04|    宮城県| 大正|         9|      1920|null|      961768|      485309|\n|            05|    秋田県| 大正|         9|      1920|null|      898537|      453682|\n|            06|    山形県| 大正|         9|      1920|null|      968925|      478328|\n|            07|    福島県| 大正|         9|      1920|null|     1362750|      673525|\n|            08|    茨城県| 大正|         9|      1920|null|     1350400|      662128|\n|            09|    栃木県| 大正|         9|      1920|null|     1046479|      514255|\n|            10|    群馬県| 大正|         9|      1920|null|     1052610|      514106|\n|            11|    埼玉県| 大正|         9|      1920|null|     1319533|      641161|\n|            12|    千葉県| 大正|         9|      1920|null|     1336155|      656968|\n|            13|    東京都| 大正|         9|      1920|null|     3699428|     1952989|\n|            14|  神奈川県| 大正|         9|      1920|null|     1323390|      689751|\n|            15|    新潟県| 大正|         9|      1920|null|     1776474|      871532|\n|            16|    富山県| 大正|         9|      1920|null|      724276|      354775|\n|            17|    石川県| 大正|         9|      1920|null|      747360|      364375|\n|            18|    福井県| 大正|         9|      1920|null|      599155|      293181|\n+--------------+----------+-----+----------+----------+----+------------+------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "#うーん使いづらい。。(日本語))\n",
    "from pyspark.sql.types import LongType, StructType, StructField, StringType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "#スキーマ設定をしていきましょう\n",
    "# カラム名、型、デフォルト値で設定していきます\n",
    "struct = StructType([\n",
    "    StructField(\"code\", StringType(), False),\n",
    "    StructField(\"kenmei\", StringType(), False),\n",
    "    StructField(\"gengo\", StringType(), False),\n",
    "    StructField(\"wareki\", StringType(), False),\n",
    "    StructField(\"seireki\", StringType(), False),\n",
    "    StructField(\"chu\", StringType(), False),\n",
    "    StructField(\"jinko_male\", StringType(), False),\n",
    "    StructField(\"jinko_female\", StringType(), False)\n",
    "])\n",
    "df=spark.read.option(\"multiLine\", \"true\").option(\"encoding\", \"SJIS\").csv(\"/Users/yuki/pyspark_batch/dataset/jinko.csv\", header=False, sep=',', inferSchema=False,schema=struct)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "metadata": {},
     "execution_count": 104
    }
   ],
   "source": [
    "# 大正や昭はもう不要かなと感じたら変換処理にて\n",
    "# ヘッダーも同時に除外してしまう\n",
    "df.where(df.gengo == \"平成\").count()\n",
    "# where 以外にもfilterと呼ばれるものがあります。機能は同じなので好きな方を選んで大丈夫です\n",
    "df.filter(df.gengo == \"平成\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------------------+--------------------+--------------------+\n|                kenmei|            male_avg|          female_avg|\n+----------------------+--------------------+--------------------+\n|人口集中地区以外の地区|4.3248248333333336E7|2.0976203166666668E7|\n|                佐賀県|            864635.0|            408192.5|\n|                栃木県|  1987157.3333333333|   987741.8333333334|\n|                京都府|  2628424.6666666665|  1268325.3333333333|\n|                香川県|           1009635.5|   485871.8333333333|\n|                愛媛県|           1466564.5|   692188.3333333334|\n|                秋田県|  1147506.8333333333|   542928.3333333334|\n|                広島県|  2865315.3333333335|           1387308.5|\n|                宮崎県|           1151179.5|            542386.5|\n|              鹿児島県|           1747640.0|            818506.0|\n|                埼玉県|   6936328.166666667|  3492880.3333333335|\n|                三重県|  1838127.1666666667|            893167.5|\n|                島根県|   744656.1666666666|            356034.5|\n|                徳島県|   806551.1666666666|   383399.1666666667|\n|                岐阜県|  2082414.3333333333|  1009389.1666666666|\n|                新潟県|  2424808.8333333335|           1175463.5|\n|                山形県|  1211415.1666666667|            583603.5|\n|              神奈川県|           8613734.5|           4360756.0|\n|                群馬県|  1999995.8333333333|   986385.1666666666|\n|                岩手県|  1374565.8333333333|   659592.6666666666|\n+----------------------+--------------------+--------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "#集計をしてみます\n",
    "#平成の県ごとの男女の数の平均\n",
    "import pyspark.sql.functions as sf\n",
    "df.where(df.gengo == \"平成\").groupBy(\"kenmei\").agg(sf.avg(\"jinko_male\").alias(\"male_avg\"),sf.avg(\"jinko_female\").alias(\"female_avg\")).show()\n",
    "\n",
    "#「人口集中地区以外の地区」が邪魔ですね。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------+------------------+-----------------+\n|  kenmei|          male_avg|       female_avg|\n+--------+------------------+-----------------+\n|  鳥取県| 602176.6666666666|287885.3333333333|\n|  島根県| 744656.1666666666|         356034.5|\n|  高知県| 790785.1666666666|372268.1666666667|\n|  徳島県| 806551.1666666666|383399.1666666667|\n|  福井県| 815695.1666666666|         395512.5|\n|  佐賀県|          864635.0|         408192.5|\n|  山梨県|          867609.0|425777.8333333333|\n|  香川県|         1009635.5|485871.8333333333|\n|和歌山県|1037736.3333333334|         490624.0|\n|  富山県|1105906.8333333333|         532857.0|\n|  秋田県|1147506.8333333333|542928.3333333334|\n|  宮崎県|         1151179.5|         542386.5|\n|  石川県|         1170582.5|         566064.0|\n|  大分県|1210304.3333333333|571530.6666666666|\n|  山形県|1211415.1666666667|         583603.5|\n|  沖縄県|1333672.6666666667|         654622.0|\n|  滋賀県|         1342717.0|         662391.0|\n|  岩手県|1374565.8333333333|659592.6666666666|\n|  奈良県|1405915.3333333333|671178.6666666666|\n|  青森県|1426420.8333333333|675238.6666666666|\n+--------+------------------+-----------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "#良さそうです！\n",
    "df.where(df.gengo == \"平成\").groupBy(\"kenmei\").agg(sf.avg(\"jinko_male\").alias(\"male_avg\"),sf.avg(\"jinko_female\").alias(\"female_avg\")).filter(df.kenmei != \"人口集中地区以外の地区\").sort(\"male_avg\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#結果を一度保存しておきます\n",
    "df_after_t=df.where(df.gengo == \"平成\").groupBy(\"kenmei\").agg(sf.avg(\"jinko_male\").alias(\"male_avg\"),sf.avg(\"jinko_female\").alias(\"female_avg\")).filter(df.kenmei != \"人口集中地区以外の地区\").sort(\"male_avg\")"
   ]
  },
  {
   "source": [
    "# DIKWモデル\n",
    "少し脇道にそれるのですが、上記の作業はDIKWモデルというものに沿った動きでです。\n",
    "\n",
    "DIKWモデルでは、データのステージを「Data」「Infromation」「Knowledge」「Wisdom」として定義しています。\n",
    "\n",
    "- Data(データ)\n",
    "- Information（情報）\n",
    "- Knowledge（知識）\n",
    "- Wisdom（知恵）\n",
    "\n",
    "これらの頭文字をとってDIKWモデルと呼ばれています。\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark利用の停止\n",
    "spark.stop()\n",
    "spark.sparkContext.stop()"
   ]
  }
 ]
}