{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.0 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "cadefa720d1a2267f4d12d08d812560a64cfe891877bc388bf0e8af3e4846067"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "![図1.2 Sparkの紹介とインストール](images/introduce_spark.png)\n",
    "\n",
    "# 本チャプターの目次\n",
    "1. Sparkとは？\n",
    "2. 分散処理とは？\n",
    "3. PySparkとは？\n",
    "4. Pyspark インストール\n",
    "5. Pysparkを簡単に動かしてみよう"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Sparkとは？\n",
    "複数のノード(PC)でデータを読み込んで、データを処理する分散コンピューティングエンジンです。  \n",
    "大きなデータを処理するときには、一台で処理を行うよりも効率が良く物事を処理することができるようになります。  \n",
    "中身はPythonで利用する関数や変数などを利用することもできます。  \n",
    "\n",
    "今回のSparkのバージョンは3.1.1を利用します。  \n",
    " \n",
    "Sparkは世の中ABC人材(AI,BigData,Cloud)な人材になるための必須スキルと言っても過言ではありません。  \n",
    "Sparkがスキルセットに存在しているだけで、企業のデータ活用の人材として重宝されます。\n",
    "\n",
    "Sparkはコンピューティングエンジンで特定のプログラミング言語に対して、分散処理能力を与えます。\n",
    "対象のプログラミング言語は\n",
    "\n",
    "1. Scala\n",
    "2. Python\n",
    "3. Java\n",
    "の３つです。\n",
    "\n",
    "PythonとSparkを組み合わせて使うとPythonが分散処理の能力を持ちます。  \n",
    "\n",
    "そしてPythonとSparkなのでPysparkと呼ばれています。  \n",
    "本コースでは、あまり内部的な構造には踏み込みませんが、次のチャプターで簡単にSparkの機能について紹介します。 "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 分散処理とは？\n",
    "大きなテキストがあって、 そのテキストを複数の人間(パソコンで仕事を分担)すること。\n",
    "\n",
    "![図1.2 分散処理](images/bunsan.png)\n",
    "\n",
    "\n",
    "## スレッド処理との違い？\n",
    "スレッド処理は、ノードが一台で実施するのに対して、分散処理は複数ノードで実施されることが特徴  \n",
    "スレッド処理はスケールアップ型(一台のPCのメモリやCPUを増強していくこと)  \n",
    "分散処理はスケールアウト型(スペックがそこそこ(そこそこ、と言ってもお家で使っているようなPCではない)なノード複数台で一つの目的のために処理を行うこと)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# PySparkとは？\n",
    "PythonとSparkを組み合わせて使うとPythonが分散処理の能力を持ちます。  \n",
    "そしてPythonとSparkなのでPysparkと呼ばれています。  \n",
    "\n",
    "Pythonのプログラムも当然かけます。 \n",
    "\n",
    "日に数万Jobというビッグデータ処理でも難なくこなします。\n",
    "\n",
    "## Scala + Sparkとの比較は？\n",
    "(かなりの)速度を求めるのであればScalaになります(メモリ効率はPythonより断然にScala(or Java))。  \n",
    "容易に使う用途であればPythonになります。  \n",
    "また、他のプロダクトがSparkをサポートする場合はScalaの方から実装されることが多く、数ヶ月してPythonが実装されるという流れが大半です。  \n",
    "\n",
    "そのため、PySparkは主にバッチ処理で実装するのがおすすめで、ストリーミング処理はScala(or Java)で実装してみるのが良いと思います。\n",
    "\n",
    "## PySparkでできること\n",
    "1. RDDの操作\n",
    "2. バッチ処理(DataFrameの操作、SQL、機械学習、不正検、AI、受給予測)\n",
    "3. データを(だいたい)一行づつ処理していくStreaming処理(DataFrameの操作、SQL、IoT,不正検知、機械学習、AI、受給予測)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Pyspark インストール\n",
    "\n",
    "## VSCodeのインストール\n",
    "以下のURLからご自身のOSに合わせたVSCodeをダウンロード  \n",
    "https://code.visualstudio.com/download"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## VSCodeのセッティング\n",
    "1. Python拡張機能をインストールする(cmd + shift + p) -> install extensions -> ウィンドウにpythonと入力\n",
    "2. cmd + shift + p -> python interpreter -> python3.8\n",
    "3. cmd + shift + p -> create ju -> Pythonが利用可能なノートブック環境ができあがります"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pythonコードのテスト\n",
    "1+2\n",
    "#実行はctr + enterで可能です"
   ]
  },
  {
   "source": [
    "## PySparkのセッティング\n",
    "1. !pip3 install pyspark --user jupyter　を実行"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install pyspark --user jupyter\n",
    "# !コマンドでLinux(Mac or winwods)のコマンドを発行することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # 現在のディレクトリを調べる\n",
    "!pwd"
   ]
  },
  {
   "source": [
    "#pysparkに必要なライブラリを読み込む\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#spark sessionの作成\n",
    "# spark.ui.enabled trueとするとSparkのGUI画面を確認することができます\n",
    "# spark.eventLog.enabled true　とすると　GUIで実行ログを確認することができます\n",
    "# GUIなどの確認は次のチャプターで説明を行います。\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"chapter1\") \\\n",
    "    .config(\"hive.exec.dynamic.partition\", \"true\") \\\n",
    "    .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"JST\") \\\n",
    "    .config(\"spark.ui.enabled\",\"true\") \\\n",
    "    .config(\"spark.eventLog.enabled\",\"true\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.sql(\"select 2 as a ,'hoge' as b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 件数をとったり\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrameを操作したりできます\n",
    "df.select(\"b\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark利用の停止(最後は停止を忘れずに)\n",
    "spark.stop()\n",
    "spark.sparkContext.stop()"
   ]
  }
 ]
}