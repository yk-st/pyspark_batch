{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![図1.2 Sparkの紹介とインストール](images/introduce_spark.png)\n",
    "\n",
    "# 本チャプターの目次\n",
    "1. Sparkとは？\n",
    "2. 分散処理とは？\n",
    "3. PySparkとは？\n",
    "4. Pyspark インストールとノートブックの簡単な説明\n",
    "5. Pysparkを簡単に動かしてみよう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkとは？\n",
    "複数のノード(PC)でデータを読み込んで、データを処理する分散コンピューティングエンジンです。  \n",
    "大きなデータを処理するときには、一台で処理を行うよりも複数台で行うほうが、効率が良く物事を処理することができるようになります。  \n",
    "\n",
    "Sparkは世の中ABC人材(AI,BigData,Cloud)な人材になるための必須スキルと言っても過言ではありません。  \n",
    "Sparkがスキルセットに存在しているだけで、企業のデータ活用の人材として重宝されます。\n",
    "\n",
    "Sparkはコンピューティングエンジンで特定のプログラミング言語に対して、分散処理能力を与えます。\n",
    "対象のプログラミング言語は\n",
    "\n",
    "1. Scala\n",
    "2. Python\n",
    "3. Java\n",
    "\n",
    "\n",
    "の３つです。\n",
    "\n",
    "PythonとSparkを組み合わせて使うとPythonが分散処理の能力を持ちます\n",
    "そしてPythonとSparkなのでPysparkと呼ばれています。\n",
    "Sparkはただのエンジンなので、PySparkでPythonが利用する関数や変数などを利用することもできます。  \n",
    "\n",
    "本コースでは、あまり内部的な構造には踏み込みませんが、次の次のレクチャーで簡単にSparkの機能について紹介します。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分散処理とは？\n",
    "複数のノードに分けて処理を分けて行うことです。  \n",
    "人間に例えるのであれば、一つの仕事を複数人で分担して行うイメージを持ってもらえればと思います。  \n",
    "\n",
    "今回は例として、特定文字(am と I)文字数のカウントを考えてみようと思います。  \n",
    "大きなテキストがあって、そのテキストを複数の人間(パソコンで仕事を分担)すること。\n",
    "\n",
    "![図1.2 分散処理](images/bunsan.png)\n",
    "\n",
    "\n",
    "## スレッド処理との違い？\n",
    "スレッド処理は、ノードが一台で実施するのに対して、分散処理は複数ノードで実施されることが特徴  \n",
    "スレッド処理はスケールアップ型(一台のPCのメモリやCPUを増強していくこと)  \n",
    "分散処理はスケールアウト型(スペックがそこそこ(そこそこ、と言ってもお家で使っているようなPCではない)なノード複数台で一つの目的のために処理を行うこと)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySparkとは？\n",
    "PythonとSparkを組み合わせて使うとPythonが分散処理の能力を持ちます。  \n",
    "そしてPythonとSparkなのでPysparkと呼ばれています。  \n",
    "\n",
    "特徴\n",
    "- Pythonのプログラムも当然かけます。 \n",
    "- 日に数万Jobというビッグデータ処理でも難なくこなします。\n",
    "\n",
    "今回のSparkのバージョンは3.1.1を利用します。 \n",
    "\n",
    "## Scala + Sparkとの比較は？\n",
    "(かなりの)速度を求めるのであればScalaになります(メモリ効率はPythonより断然にScala(or Java))。  \n",
    "容易に使う用途であればPythonになります。  \n",
    "\n",
    "### Scala + Sparkを使ったほうがいい場面\n",
    "- UDFを作成する時\n",
    "- のロダクトがSparkをサポートする場合はScalaの方から実装されることが多く、数ヶ月してPythonが実装されるという流れが大半です\n",
    "\n",
    "## PySparkでできること\n",
    "1. RDDの操作\n",
    "2. バッチ処理(DataFrameの操作、SQL、機械学習、不正検、AI、受給予測)\n",
    "3. データを(だいたい)一行づつ処理していくStreaming処理(DataFrameの操作、SQL、IoT,不正検知、機械学習、AI、受給予測)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/yuki/pyspark_batch\n"
     ]
    }
   ],
   "source": [
    " # 現在のディレクトリを調べる\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/11 10:13:52 WARN SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor). This may indicate an error, since only one SparkContext should be running in this JVM (see SPARK-2243). The other SparkContext was created at:\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "java.base/java.lang.Thread.run(Thread.java:830)\n"
     ]
    }
   ],
   "source": [
    "#pysparkに必要なライブラリを読み込む\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#spark sessionの作成\n",
    "# spark.ui.enabled trueとするとSparkのGUI画面を確認することができます\n",
    "# spark.eventLog.enabled true　とすると　GUIで実行ログを確認することができます\n",
    "# GUIなどの確認は最後のセクションで説明を行います。\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"chapter1\") \\\n",
    "    .config(\"hive.exec.dynamic.partition\", \"true\") \\\n",
    "    .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"JST\") \\\n",
    "    .config(\"spark.ui.enabled\",\"true\") \\\n",
    "    .config(\"spark.eventLog.enabled\",\"true\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# spark.xxxxxと記載することで処理を分散させることが可能です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.sql(\"select 2 as a ,'hoge' as b\")\n",
    "\n",
    "# あまり実感がわかないかもしれませんが、複数ノードで実行すれば、この時点で処理が分散されています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 分散処理を行いながら件数をとったり\n",
    "df.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|   b|\n",
      "+----+\n",
      "|hoge|\n",
      "+----+\n",
      "\n",
      "+---+----+\n",
      "|  a|   b|\n",
      "+---+----+\n",
      "|  2|hoge|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DataFrameを操作したりできます\n",
    "df.select(\"b\").show()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark利用の停止(最後は停止を忘れずに)\n",
    "spark.stop()\n",
    "spark.sparkContext.stop()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cadefa720d1a2267f4d12d08d812560a64cfe891877bc388bf0e8af3e4846067"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
