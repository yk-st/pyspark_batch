{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.0 64-bit"
  },
  "interpreter": {
   "hash": "cadefa720d1a2267f4d12d08d812560a64cfe891877bc388bf0e8af3e4846067"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "![図1.2 Sparkの紹介とインストール](images/introduce_spark.png)\n",
    "\n",
    "# 本チャプターの目次\n",
    "1. Sparkとは？\n",
    "2. 分散処理とは？\n",
    "3. PySparkとは？\n",
    "4. Pyspark インストールとノートブックの簡単な説明\n",
    "5. Pysparkを簡単に動かしてみよう"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sparkとは？\n",
    "複数のノード(PC)でデータを読み込んで、データを処理する分散コンピューティングエンジンです。  \n",
    "大きなデータを処理するときには、一台で処理を行うよりも複数台で行うほうが、効率が良く物事を処理することができるようになります。  \n",
    "\n",
    "Sparkは世の中ABC人材(AI,BigData,Cloud)な人材になるための必須スキルと言っても過言ではありません。  \n",
    "Sparkがスキルセットに存在しているだけで、企業のデータ活用の人材として重宝されます。\n",
    "\n",
    "Sparkはコンピューティングエンジンで特定のプログラミング言語に対して、分散処理能力を与えます。\n",
    "対象のプログラミング言語は\n",
    "\n",
    "1. Scala\n",
    "2. Python\n",
    "3. Java\n",
    "\n",
    "\n",
    "の３つです。\n",
    "\n",
    "PythonとSparkを組み合わせて使うとPythonが分散処理の能力を持ちます\n",
    "そしてPythonとSparkなのでPysparkと呼ばれています。\n",
    "Sparkはただのエンジンなので、PySparkでPythonが利用する関数や変数などを利用することもできます。  \n",
    "\n",
    "本コースでは、あまり内部的な構造には踏み込みませんが、次の次のレクチャーで簡単にSparkの機能について紹介します。 "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 分散処理とは？\n",
    "複数のノードに分けて処理を分けて行うことです。  \n",
    "人間に例えるのであれば、一つの仕事を複数人で分担して行うイメージを持ってもらえればと思います。  \n",
    "\n",
    "今回は例として、特定文字(am と I)文字数のカウントを考えてみようと思います。  \n",
    "大きなテキストがあって、そのテキストを複数の人間(パソコンで仕事を分担)すること。\n",
    "\n",
    "![図1.2 分散処理](images/bunsan.png)\n",
    "\n",
    "\n",
    "## スレッド処理との違い？\n",
    "スレッド処理は、ノードが一台で実施するのに対して、分散処理は複数ノードで実施されることが特徴  \n",
    "スレッド処理はスケールアップ型(一台のPCのメモリやCPUを増強していくこと)  \n",
    "分散処理はスケールアウト型(スペックがそこそこ(そこそこ、と言ってもお家で使っているようなPCではない)なノード複数台で一つの目的のために処理を行うこと)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PySparkとは？\n",
    "PythonとSparkを組み合わせて使うとPythonが分散処理の能力を持ちます。  \n",
    "そしてPythonとSparkなのでPysparkと呼ばれています。  \n",
    "\n",
    "特徴\n",
    "- Pythonのプログラムも当然かけます。 \n",
    "- 日に数万Jobというビッグデータ処理でも難なくこなします。\n",
    "\n",
    "今回のSparkのバージョンは3.1.1を利用します。 \n",
    "\n",
    "## Scala + Sparkとの比較は？\n",
    "(かなりの)速度を求めるのであればScalaになります(メモリ効率はPythonより断然にScala(or Java))。  \n",
    "容易に使う用途であればPythonになります。  \n",
    "\n",
    "### Scala + Sparkを使ったほうがいい場面\n",
    "- UDFを作成する時\n",
    "- のロダクトがSparkをサポートする場合はScalaの方から実装されることが多く、数ヶ月してPythonが実装されるという流れが大半です\n",
    "\n",
    "## PySparkでできること\n",
    "1. RDDの操作\n",
    "2. バッチ処理(DataFrameの操作、SQL、機械学習、不正検、AI、受給予測)\n",
    "3. データを(だいたい)一行づつ処理していくStreaming処理(DataFrameの操作、SQL、IoT,不正検知、機械学習、AI、受給予測)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pyspark インストール\n",
    "\n",
    "## VSCodeのインストール\n",
    "以下のURLからご自身のOSに合わせたVSCodeをダウンロード  \n",
    "https://code.visualstudio.com/download"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## VSCodeのセッティング\n",
    "\n",
    "1. Python拡張機能をインストールする(cmd + shift + p) -> install extensions -> ウィンドウにpythonと入力\n",
    "2. cmd + shift + p -> python interpreter -> python3.8\n",
    "3. cmd + shift + p -> create no -> Pythonが利用可能なノートブック環境ができあがります"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "#Pythonコードのテスト\n",
    "1+2\n",
    "#実行はctr + enterで可能です"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PySparkのセッティング\n",
    "1. !pip3 install pyspark --user jupyter　を実行"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip3 install pyspark --user jupyter\n",
    "# !コマンドでLinux(Mac or winwods)のコマンドを発行することができます。"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    " # 現在のディレクトリを調べる\n",
    "!pwd"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#pysparkに必要なライブラリを読み込む\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#spark sessionの作成\n",
    "# spark.ui.enabled trueとするとSparkのGUI画面を確認することができます\n",
    "# spark.eventLog.enabled true　とすると　GUIで実行ログを確認することができます\n",
    "# GUIなどの確認は最後のセクションで説明を行います。\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"chapter1\") \\\n",
    "    .config(\"hive.exec.dynamic.partition\", \"true\") \\\n",
    "    .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"JST\") \\\n",
    "    .config(\"spark.ui.enabled\",\"true\") \\\n",
    "    .config(\"spark.eventLog.enabled\",\"true\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# spark.xxxxxと記載することで処理を分散させることが可能です。"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df=spark.sql(\"select 2 as a ,'hoge' as b\")\n",
    "\n",
    "# あまり実感がわかないかもしれませんが、複数ノードで実行すれば、この時点で処理が分散されています。"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 分散処理を行いながら件数をとったり\n",
    "df.count()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# DataFrameを操作したりできます\n",
    "df.select(\"b\").show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Spark利用の停止(最後は停止を忘れずに)\n",
    "spark.stop()\n",
    "spark.sparkContext.stop()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}