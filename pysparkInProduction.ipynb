{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![図1.2 Sparkの紹介とインストール](images/PysparkInProduction.png)\n",
    "\n",
    "# 本セクションの目次\n",
    "\n",
    "1. 本番で動かすことを前提とした、これまでと本レクチャーの違い\n",
    "2. Pysparkを本番環境で動かす際の流れ\n",
    "3. Spark Submitを動かす(Sparkをコマンドラインで実行する方法)\n",
    "4. チューニングのコツ\n",
    "   1. Spark Webインタフェースを用いたボトルネックの調査\n",
    "   2. メモリへの登録"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkを用いたバッチにおけるデータエンジニアリング一連の流れ(復習)\n",
    "\n",
    "1. データソースの読み込み(今回は、人口統計データ(/dataset/jinkou.csv))　ETL(Extract Transform Load)で言うEの部分\n",
    "2. 変換を行う(集計等を行う)　DataFrame処理 or SQL処理の２パターンで実行可能 ETL(Extract Transform Load)で言うTの部分\n",
    "3. カラムナーフォーマットへ変換する ETL(Extract Transform Load)で言うTの部分\n",
    "4. 出力したデータをみんなに見やすくするため(BIツールから参照できるように)テーブルを作成する ETL(Extract Transform Load)で言うLの部分\n",
    "\n",
    "本チャプターではこの流れを、Spark-Submitを使った方法で実行してみようと思います。\n",
    "ただし、テーブルは事前に作成しておくことにします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 本番で動かすことを前提とした、これまでと本セクションの違い\n",
    "これまでは、ローカル環境での開発を想定していました。  \n",
    "本レクチャーでは、本番環境で実行をするための準備を行っていきます。  \n",
    "\n",
    "セクション3,4で行ってきた内容を、本番環境に適用することを考えていきます。\n",
    "\n",
    "主に使うリソースは./pysaprk_batch/spark_etlフォルダ配下に入っています。  \n",
    "\n",
    "# 各リソースの説明\n",
    "- etl.sql(sparkSQL。データの変換を担当するSQL。セクション4で紹介した処理と同じだが、元号だけあえて可変引数にしている。spark_etl_sample.pyで読み込んで実行する)\n",
    "- jinko_schema.json(jinko.csvのスキーマ定義が入っている。動的に行うためにはファイルで外出しすることが好ましい。spark_etl_sample.pyで読み込んで適用する)\n",
    "- spark_etl_sample.py(データエンジニアリング一覧の流れが記載されている)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 環境の作成\n",
    "\n",
    "もしPython3がインストールされていない場合は、以下からインストールをしましょう\n",
    "https://www.python.org/downloads/release/python-381/\n",
    "\n",
    "1. ターミナルを開きます。\n",
    "2. インストールしたらpip3 install pysparkを実行しましょう\n",
    "3. spark-submitのコマンドが出てくれば成功です"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySparkを実際に本番環境で動かす際の流れ\n",
    "ここからのメインのトピックとしては、\n",
    "\n",
    "- スケジューラーで実行するために、spark_etl_sample.pyをコマンドベース(ターミナルを本番環境と考えてくだし)でどのように動かすか？\n",
    "- 本番で直面する問題や、チューニングする際に利用するツールやそのコツ\n",
    "\n",
    "を紹介していきます。\n",
    "\n",
    "注。　ここからNoteBookに移動したり、はたまたターミナルに移動したりします。ご了承ください"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# metastore_dbのロックを外します\n",
    "\n",
    "削除するもの\n",
    "- metasotre_db/db.lck\n",
    "- metastore_db/dbex.lck\n",
    "\n",
    "ローカル環境では、複数の環境(ノートブック、ターミナル)からテーブル定義は共有して利用できても、同時に接続することができません。  \n",
    "そのため、ノートブックのロックを解除して上げる必要があります(metastore_db限定なのでMySQLやAmazon Glue Data Catalogなどはこのようなことをする必要ありません)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spark-submit でSparkを実行する\n",
    "\n",
    "コマンドラインでSparkを実行するにはこの方法しかない。\n",
    "構文としては\n",
    "\n",
    "```\n",
    "spark-submit [メモリ設定のオプションなど] spark_etl_sample.py 引数 \n",
    "```\n",
    "\n",
    "で実行可能です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spark_etl_sample.pyを実行していきます\n",
    "# ここからはノートブックではなくpythonエディター（ターミナル）へ移ります。\n",
    "\n",
    "ターミナルを起動したらmetastore_dbが直下にあるところまで移動をしてください。\n",
    "メタデータストアなので、ノートブックから作ったテーブル定義(jinko_avg)を利用することが可能だからです(今回だとjinko_avgのテーブル定義を使います)。\n",
    "\n",
    "```\n",
    "spark-submit ./spark_etl/spark_etl_sample.py -a hoge -s ./spark_etl/etl.sql -b GENGO=昭和 -t jinko -z ./spark_etl/jinko_schema.json -f /Users/yuki/pyspark_batch/dataset/jinko.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pysparkに必要なライブラリを読み込む\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#spark sessionの作成\n",
    "# spark.ui.enabled trueとするとSparkのGUI画面を確認することができます\n",
    "# spark.eventLog.enabled true　とすると　GUIで実行ログを確認することができます\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"chapter5\") \\\n",
    "    .config(\"hive.exec.dynamic.partition\", \"true\") \\\n",
    "    .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"JST\") \\\n",
    "    .config(\"spark.ui.enabled\",\"true\") \\\n",
    "    .config(\"spark.eventLog.enabled\",\"true\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from jinko_avg\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実際にクラウド上の実行環境でspark-submitを行う場合は、実行の設定値がラップされていることが多いです。\n",
    "あまり実行において気にすることはあまりない  \n",
    "オンプレの場合は、細かな設定を行わなければならず苦労するポイントかも知れません。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# チューニングTips\n",
    "\n",
    "Sparkというと、メモリに気をとられドライバー(Sparkの実行を指示する親玉)のメモリーやエグゼキューター(実行部隊)のメモリについてのチューニング記事がたくさん出てきます。  \n",
    "しかしながら、これらのメモリー設定がどうしても必要になるパターンはまれです(昔はよく設定していた)。\n",
    "\n",
    "メモリー設定を弄りまわす前に確認したいいくつかのポイントを紹介します\n",
    "\n",
    "1. Spark web インタフェースでのボトルネックの確認\n",
    "2. メモリへの登録\n",
    "3. repartition数を大きくしてみる\n",
    "4. Executor(ノード)の台数を増やす(クラウドであれば容易)　メモリ設定などですごく迷うのであれば、さっとノード追加して動かしてしまったほうが結局安上がり\n",
    "\n",
    "\n",
    "※Sparkのプログラムはエグゼキューターと呼ばれる環境内で実行されます。Sparkのプログラムが実行されるとドライバーがエグゼギューターを立ち上げ実行をエグゼキューターに指示しエグゼキューターが実行を行います。  \n",
    "今回はドライバーとエグゼキューターはローカル端末に同居しています。複数端末の場合はエグゼキューターが様々な端末で起動します。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark web インタフェースでのボトルネックの確認\n",
    "Sparkのウェブインタフェースを眺めてみましょう\n",
    "\n",
    "1. sparksessionを起動する\n",
    "2. http://localhost:4040 へアクセスしま\n",
    "\n",
    "まずは簡単な画面説明から。\n",
    "\n",
    "いくつかの処理を実行して、結果を見比べてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import LongType, StructType, StructField, StringType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# カラム名、型、デフォルト値で設定していきます\n",
    "struct = StructType([\n",
    "    StructField(\"code\", StringType(), False),\n",
    "    StructField(\"kenmei\", StringType(), False),\n",
    "    StructField(\"gengo\", StringType(), False),\n",
    "    StructField(\"wareki\", StringType(), False),\n",
    "    StructField(\"seireki\", StringType(), False),\n",
    "    StructField(\"chu\", StringType(), False),\n",
    "    StructField(\"sokei\", StringType(), False),\n",
    "    StructField(\"jinko_male\", StringType(), False),\n",
    "    StructField(\"jinko_female\", StringType(), False)\n",
    "])\n",
    "df=spark.read.option(\"multiLine\", \"true\") \\\n",
    "    .option(\"encoding\", \"SJIS\").csv(\"/Users/yuki/pyspark_batch/dataset/jinko.csv\", header=False, sep=',', inferSchema=False,schema=struct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\" \n",
    "\n",
    "select * from jinko_avg\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\" \n",
    "\n",
    "select * from jinko_avg where kenmei='三重県'\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\" \n",
    "\n",
    "select * from jinko_avg where male_avg > 3000000    \n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# メモリへの登録\n",
    "処理途中のデータはメモリへ登録することが可能です。  \n",
    "何度も使い回す場合で比較的小さいデータはメモリ登録することで速度の向上が見込めます。\n",
    "\n",
    "メモリの登録にはいくつか種類があります\n",
    "\n",
    "1. dataframeをキャッシュする\n",
    "2. (テンポラリ)テーブルをキャッシュする\n",
    "\n",
    "\n",
    "※メモリ以外にも、ディスクに書き出すという方法もあります(persist()と呼ばれるもの、MEMORY_AND_DISKやDISK_ONLYといった形でキャッシュが可能。persist(StorageLevel.MEMORY_AND_DISK)といった書き方をする)。  \n",
    "※チェックポイントと呼ばれる機能もありますが、相当長いアプリケーションでないと特徴を活かせません  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframeをキャッシュする場合\n",
    "df=spark.sql(\"\"\" \n",
    "\n",
    "select * from jinko_avg where kenmei='三重県'\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "# キャッシュする\n",
    "df.cache()\n",
    "# キャッシュ判定\n",
    "df.is_cached\n",
    "\n",
    "# 後続でdfを使うときはキャッシュから利用される\n",
    "df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#キャッシュのクリア\n",
    "df.unpersist()\n",
    "\n",
    "df.is_cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テーブルをキャッシュすることも可能です\n",
    "spark.catalog.cacheTable(\"jinko_avg\")\n",
    "\n",
    "# キャッシュの有無\n",
    "spark.catalog.isCached(\"jinko_avg\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# キャッシュのクリア\n",
    "spark.catalog.uncacheTable(\"jinko_avg\")\n",
    "\n",
    "# キャッシュの有無\n",
    "spark.catalog.isCached(\"jinko_avg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark利用の停止\n",
    "spark.stop()\n",
    "spark.sparkContext.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 演習テスト1の回答\n",
    "\n",
    "```\n",
    "spark-submit ./spark_etl/spark_etl_sample.py -a hoge -s ./spark_etl/etl.sql -b GENGO=大正 -t jinko -z ./spark_etl/jinko_schema.json -f /Users/yuki/pyspark_batch/dataset/jinko.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 演習テスト2の回答\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.uncacheTable(\"jinko_avg\")\n",
    "spark.catalog.isCached(\"jinko_avg\")\n",
    "\n",
    "if not spark.catalog.isCached(\"jinko_avg\"):\n",
    "  spark.catalog.cacheTable(\"jinko_avg\")\n",
    "  print(spark.sql(\"select * from jinko_avg\").count())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cadefa720d1a2267f4d12d08d812560a64cfe891877bc388bf0e8af3e4846067"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
