{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![図1.2 Sparkの紹介とインストール](images/PysparkInProduction.png)\n",
    "\n",
    "# 本チャプターの目次\n",
    "\n",
    "1. これまでと本レクチャーの違い\n",
    "2. Pysparkを本番環境で動かす際の流れ\n",
    "3. Spark Submitを動かす(Sparkをコマンドラインで実行する方法)\n",
    "4. チューニングのコツ\n",
    "   1. Spark Webインタフェースを用いたボトルネックの作成\n",
    "   2. メモリへの登録"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkを用いたバッチにおけるデータエンジニアリング一連の流れ(復習)\n",
    "\n",
    "1. データソースの読み込み(今回は、人口統計データ(/dataset/jinkou.csv))　ETL(Extract Transform Load)で言うEの部分\n",
    "2. 変換を行う(集計等を行う)　DataFrame処理 or SQL処理の２パターンで実行可能 ETL(Extract Transform Load)で言うTの部分\n",
    "3. カラムナーフォーマットへ変換する ETL(Extract Transform Load)で言うTの部分\n",
    "4. 出力したデータをみんなに見やすくするため(BIツールから参照できるように)テーブルを作成する ETL(Extract Transform Load)で言うLの部分\n",
    "\n",
    "本チャプターではこの流れを、Spark-Submitを使った方法で実行してみようと思います。\n",
    "ただし、テーブルは事前に作成しておくことにします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# これまでと本レクチャーの違い\n",
    "これまでは、ローカル環境での開発を想定していました。  \n",
    "本レクチャーでは、本番環境で実行をするための準備を行っていきます。  \n",
    "\n",
    "セクション3,4で行ってきた内容を、本番環境に適用することを考えていきます。\n",
    "\n",
    "主に使うリソースは./pysaprk_batch/spark_etlフォルダ配下に入っています。  \n",
    "\n",
    "# 各リソースの説明\n",
    "- etl.sql(sparkSQL。データの変換を担当するSQL。セクション4で紹介した処理と同じだが、元号だけあえて可変引数にしている。spark_etl_sample.pyで読み込んで実行する)\n",
    "- jinko_schema.json(jinko.csvのスキーマ定義が入っている。動的に行うためにはファイルで外出しすることが好ましい。spark_etl_sample.pyで読み込んで適用する)\n",
    "- spark_etl_sample.py(データエンジニアリング一覧の流れが記載されている)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySparkを実際に本番環境で動かす際の流れ\n",
    "ローカル端末で動かすことはわかったので、本チャプターはPySparkを実際に本番環境で実行するためのTipsを紹介していきます。  \n",
    "メインのトピックとしては、\n",
    "\n",
    "- スケジューラーで実行するために、sample_elt_sample.pyをコマンドベースでどのように動かすか？\n",
    "- 本番で直面する問題や、チューニングする際に利用するツールやそのコツ\n",
    "\n",
    "を紹介していきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spark-submit でSparkを実行する\n",
    "\n",
    "コマンドラインでSparkを実行するにはこの方法しかない。\n",
    "構文としては\n",
    "\n",
    "```\n",
    "spark-submit xxxxx.py 引数 \n",
    "```\n",
    "\n",
    "で実行可能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pysparkに必要なライブラリを読み込む\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#spark sessionの作成\n",
    "# spark.ui.enabled trueとするとSparkのGUI画面を確認することができます\n",
    "# spark.eventLog.enabled true　とすると　GUIで実行ログを確認することができます\n",
    "# GUIなどの確認は次のチャプターで説明を行います。\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"chapter3\") \\\n",
    "    .config(\"hive.exec.dynamic.partition\", \"true\") \\\n",
    "    .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"JST\") \\\n",
    "    .config(\"spark.ui.enabled\",\"true\") \\\n",
    "    .config(\"spark.eventLog.enabled\",\"true\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テーブルは事前に作成しておきます(テーブル名はちょっとだけ変えてjinko_avg2としてます)\n",
    "spark.sql(\"\"\" \n",
    "CREATE EXTERNAL TABLE2 IF NOT EXISTS default.jinko_avg2 ( male_avg double, female_avg double)\n",
    "PARTITIONED BY (kenmei String)\n",
    "STORED AS PARQUET\n",
    "LOCATION '/Users/yuki/pyspark_batch/dataset/parquet/';\n",
    "\"\"\")\n",
    "spark.sql(\" show tables\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spark_etl.pyを作成していきます\n",
    "# ここからはノートブックではなくpythonエディター（ターミナル）へ移ります。\n",
    "\n",
    "```\n",
    "spark-submit ./spark_etl/spark_etl_sample.py -a hoge -s ./spark_etl/etl.sql -b GENGO=昭和 -t jinko -z ./spark_etl/jinko_schema.json -f /Users/yuki/pyspark_batch/dataset/jinko.csv\n",
    "```\n",
    "\n",
    "ターミナルを起動したらmetastore_dbが直下にあるところまで移動をしてください。\n",
    "メタデータストアなので、ノートブックから作ったテーブル定義を利用することが可能だからです(今回だとjinko_avg2のテーブル定義を使います)。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実際にクラウド上の実行環境でspark-submitを行う場合は、比較的実行の設定値がラップされていることが多くあまり実行において気にすることはあまりない  \n",
    "オンプレの場合は、細かな設定を行わなければならず苦労するポイントかも知れません。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# チューニングTips\n",
    "\n",
    "Sparkというと、メモリに気をとられドライバーのメモリーやエグゼキューターのメモリについてのチューニング記事がたくさん出てきます。  \n",
    "しかしながら、これらのメモリー設定がどうしても必要になるパターンはまれです(昔はよく設定していた)。\n",
    "\n",
    "メモリー設定を弄りまわす前に確認したいいくつかのポイントを紹介します\n",
    "\n",
    "1. Spark web インタフェースでのボトルネックの確認\n",
    "2. メモリへの登録\n",
    "3. repartition数を大きくしてみる\n",
    "4. Executor(ノード)の台数を増やす(クラウドであれば容易)　メモリ設定などですごく迷うのであれば、さっとノード追加して動かしてしまったほうが結局安上がり\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark web インタフェースでのボトルネックの確認\n",
    "Sparkのウェブインタフェースを眺めてみましょう\n",
    "\n",
    "1. sparksessionを起動する\n",
    "2. http://localhost:4040 へアクセスします\n",
    "\n",
    "いくつかの処理を実行して、結果を見比べてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\" \n",
    "\n",
    "select * from jinko_avg2\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\" \n",
    "\n",
    "select * from jinko_avg2 where kenmei='三重県'\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\" \n",
    "\n",
    "select * from jinko_avg2 where male_avg > 3000000    \n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# メモリへの登録\n",
    "処理途中のデータはメモリへ登録することが可能です。  \n",
    "何度も使い回す場合で比較的小さいデータはメモリ登録することで速度の向上が見込めます。\n",
    "\n",
    "メモリの登録にはいくつか種類があります\n",
    "\n",
    "1. dataframeをキャッシュする\n",
    "2. テンポラリテーブルをキャッシュする\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframeをキャッシュする場合\n",
    "df=spark.sql(\"\"\" \n",
    "\n",
    "select * from jinko_avg2 where kenmei='三重県'\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "# キャッシュする\n",
    "df.cache()\n",
    "# キャッシュ判定\n",
    "df.is_cached\n",
    "\n",
    "# 後続でdfを使うときはキャッシュから利用される"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark利用の停止\n",
    "spark.stop()\n",
    "spark.sparkContext.stop()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cadefa720d1a2267f4d12d08d812560a64cfe891877bc388bf0e8af3e4846067"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
