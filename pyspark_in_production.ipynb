{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.0 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "cadefa720d1a2267f4d12d08d812560a64cfe891877bc388bf0e8af3e4846067"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# PySparkを実際に本番環境で動かす際の流れ\n",
    "ローカル端末で動かすことはわかったので、本チャプターはPySparkを実際に本番環境で実行するためのTipsを紹介していきます。  \n",
    "メインのトピックとしては、\n",
    "\n",
    "- スケジューラーで実行するためにコマンドベースでどのように動かすか？\n",
    "- チューニングする際に利用するツールやそのコツ\n",
    "\n",
    "を紹介していきます。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 本チャプターの目次\n",
    "\n",
    "2. SQLファイルの読み込みと実行\n",
    "3. Spark Submit(Sparkをコマンドラインで実行する方法)\n",
    "1. Spark Web インタフェース\n",
    "5. チューニングのコツ"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# spark-submit でSparkを実行する\n",
    "\n",
    "コマンドラインでSparkを実行するにはこの方法しかない。\n",
    "構文としては\n",
    "\n",
    "```\n",
    "spark-submit xxxxx.py 引数 \n",
    "```\n",
    "\n",
    "で実行可能"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Sparkを用いたバッチにおけるデータエンジニアリング一連の流れ(復習)\n",
    "\n",
    "1. データソースの読み込み(今回は、人口統計データ(/dataset/jinkou.csv))　ETL(Extract Transform Load)で言うEの部分\n",
    "2. 変換を行う(集計等を行う)　DataFrame処理 or SQL処理の２パターンで実行可能 ETL(Extract Transform Load)で言うTの部分\n",
    "3. カラムナーフォーマットへ変換する ETL(Extract Transform Load)で言うTの部分\n",
    "4. 出力したデータをみんなに見やすくするため(BIツールから参照できるように)テーブルを作成する ETL(Extract Transform Load)で言うLの部分\n",
    "\n",
    "本チャプターではこの流れを、Spark-Submitを使った方法で実行してみようと思います。\n",
    "ただし、テーブルは事前に作成しておくことにします。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pysparkに必要なライブラリを読み込む\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#spark sessionの作成\n",
    "# spark.ui.enabled trueとするとSparkのGUI画面を確認することができます\n",
    "# spark.eventLog.enabled true　とすると　GUIで実行ログを確認することができます\n",
    "# GUIなどの確認は次のチャプターで説明を行います。\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"chapter3\") \\\n",
    "    .config(\"hive.exec.dynamic.partition\", \"true\") \\\n",
    "    .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"JST\") \\\n",
    "    .config(\"spark.ui.enabled\",\"true\") \\\n",
    "    .config(\"spark.eventLog.enabled\",\"true\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テーブルを作成しておきます(テーブル名はちょっとだけ買えてjinko_avg2としてます)\n",
    "spark.sql(\"\"\" \n",
    "CREATE EXTERNAL TABLE2 IF NOT EXISTS default.jinko_avg2 ( male_avg double, female_avg double)\n",
    "PARTITIONED BY (kenmei String)\n",
    "STORED AS PARQUET\n",
    "LOCATION '/Users/yuki/pyspark_batch/dataset/parquet/';\n",
    "\"\"\")\n",
    "spark.sql(\" show tables\").show()"
   ]
  },
  {
   "source": [
    "# spark_etl.pyを作成していきます\n",
    "# ここからはノートブックではなくpythonエディターへ移ります。\n",
    "\n",
    "```\n",
    "spark-submit ./spark_etl/spark_etl_sample.py -a hoge -s ./spark_etl/etl.sql -b GENGO=昭和 -t jinko -z ./spark_etl/jinko_schema.json -f /Users/yuki/pyspark_batch/dataset/jinko.csv\n",
    "```\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "実際にクラウド上の実行環境でspark-submitを行う場合は、比較的実行の設定値がラップされていることが多くあまり実行において気にすることはあまりない  \n",
    "オンプレの場合は、細かな設定を行わなければならず苦労するポイントかも知れません。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# チューニングが必要になったら\n",
    "Sparkのウェブインタフェースを眺めてみましょう\n",
    "\n",
    "1. sparksessionを起動する\n",
    "2. http://localhost:4040 へアクセスします"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\" \n",
    "\n",
    "select * from jinko_avg2\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\" \n",
    "\n",
    "select * from jinko_avg2 where kenmei='三重県'\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\" \n",
    "\n",
    "select * from jinko_avg2 where male_avg > 3000000    \n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark利用の停止\n",
    "spark.stop()\n",
    "spark.sparkContext.stop()"
   ]
  }
 ]
}