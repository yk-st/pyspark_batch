{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.0 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "cadefa720d1a2267f4d12d08d812560a64cfe891877bc388bf0e8af3e4846067"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# PySparkを実際に本番環境で動かす際の流れ\n",
    "ローカル端末で動かすことはわかったので、本チャプターはPySparkを実際に本番環境で実行するためのTipsを紹介していきます。  \n",
    "メインのトピックとしては、\n",
    "\n",
    "- スケジューラーで実行するためにコマンドベースでどのように動かすか？\n",
    "- チューニングする際に利用するツールやそのコツ\n",
    "\n",
    "を紹介していきます。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 本チャプターの目次\n",
    "\n",
    "2. SQLファイルの読み込みと実行\n",
    "3. Spark Submit(Sparkをコマンドラインで実行する方法)\n",
    "1. Spark Web インタフェース\n",
    "5. チューニングのコツ"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# spark-submit でSparkを実行する\n",
    "\n",
    "コマンドラインでSparkを実行するにはこの方法しかない。\n",
    "構文としては\n",
    "\n",
    "```\n",
    "spark-submit xxxxx.py 引数 \n",
    "```\n",
    "\n",
    "で実行可能"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Sparkを用いたバッチにおけるデータエンジニアリング一連の流れ(復習)\n",
    "\n",
    "1. データソースの読み込み(今回は、人口統計データ(/dataset/jinkou.csv))　ETL(Extract Transform Load)で言うEの部分\n",
    "2. 変換を行う(集計等を行う)　DataFrame処理 or SQL処理の２パターンで実行可能 ETL(Extract Transform Load)で言うTの部分\n",
    "3. カラムナーフォーマットへ変換する ETL(Extract Transform Load)で言うTの部分\n",
    "4. 出力したデータをみんなに見やすくするため(BIツールから参照できるように)テーブルを作成する ETL(Extract Transform Load)で言うLの部分\n",
    "\n",
    "本チャプターではこの流れを、Spark-Submitを使った方法で実行してみようと思います。\n",
    "ただし、テーブルは事前に作成しておくことにします。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Warning: Ignoring non-Spark config property: hive.exec.dynamic.partition.mode\n",
      "Warning: Ignoring non-Spark config property: hive.exec.dynamic.partition\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/yuki/Library/Python/3.8/lib/python/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "21/09/09 15:36:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "#pysparkに必要なライブラリを読み込む\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#spark sessionの作成\n",
    "# spark.ui.enabled trueとするとSparkのGUI画面を確認することができます\n",
    "# spark.eventLog.enabled true　とすると　GUIで実行ログを確認することができます\n",
    "# GUIなどの確認は次のチャプターで説明を行います。\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"chapter3\") \\\n",
    "    .config(\"hive.exec.dynamic.partition\", \"true\") \\\n",
    "    .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"JST\") \\\n",
    "    .config(\"spark.ui.enabled\",\"true\") \\\n",
    "    .config(\"spark.eventLog.enabled\",\"true\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------+---------+-----------+\n|database|tableName|isTemporary|\n+--------+---------+-----------+\n| default|jinko_avg|      false|\n+--------+---------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# テーブルを作成しておきます\n",
    "spark.sql(\"\"\" \n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS default.jinko_avg ( male_avg double, female_avg double)\n",
    "PARTITIONED BY (kenmei String)\n",
    "STORED AS PARQUET\n",
    "LOCATION '/Users/yuki/pyspark_batch/dataset/parquet/';\n",
    "\"\"\")\n",
    "spark.sql(\" show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark_etl.pyを作成していきます\n",
    "# ここからはノートブックではなくpythonエディターへ移ります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark利用の停止\n",
    "spark.stop()\n",
    "spark.sparkContext.stop()"
   ]
  }
 ]
}