{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.0 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "cadefa720d1a2267f4d12d08d812560a64cfe891877bc388bf0e8af3e4846067"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# PySparkを実際に本番環境で動かす際の流れ\n",
    "ローカル端末で動かすことはわかったので、本チャプターはPySparkを実際に本番環境で実行するためのTipsを紹介していきます。  \n",
    "メインのトピックとしては、\n",
    "\n",
    "- スケジューラーで実行するためにコマンドベースでどのように動かすか？\n",
    "- チューニングする際に利用するツールやそのコツ\n",
    "\n",
    "を紹介していきます。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 本チャプターの目次\n",
    "\n",
    "2. SQLファイルの読み込みと実行\n",
    "3. Spark Submit(Sparkをコマンドラインで実行する方法)\n",
    "1. Spark Web インタフェース\n",
    "5. チューニングのコツ"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# spark-submit でSparkを実行する\n",
    "\n",
    "コマンドラインでSparkを実行するにはこの方法しかない。\n",
    "構文としては\n",
    "\n",
    "```\n",
    "spark-submit xxxxx.py 引数 \n",
    "```\n",
    "\n",
    "で実行可能"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Sparkを用いたバッチにおけるデータエンジニアリング一連の流れ(復習)\n",
    "\n",
    "1. データソースの読み込み(今回は、人口統計データ(/dataset/jinkou.csv))　ETL(Extract Transform Load)で言うEの部分\n",
    "2. 変換を行う(集計等を行う)　DataFrame処理 or SQL処理の２パターンで実行可能 ETL(Extract Transform Load)で言うTの部分\n",
    "3. カラムナーフォーマットへ変換する ETL(Extract Transform Load)で言うTの部分\n",
    "4. 出力したデータをみんなに見やすくするため(BIツールから参照できるように)テーブルを作成する ETL(Extract Transform Load)で言うLの部分\n",
    "\n",
    "本チャプターではこの流れを、Spark-Submitを使った方法で実行してみようと思います。\n",
    "ただし、テーブルは事前に作成しておくことにします。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Warning: Ignoring non-Spark config property: hive.exec.dynamic.partition.mode\n",
      "Warning: Ignoring non-Spark config property: hive.exec.dynamic.partition\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/yuki/Library/Python/3.8/lib/python/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "21/09/10 10:59:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "#pysparkに必要なライブラリを読み込む\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#spark sessionの作成\n",
    "# spark.ui.enabled trueとするとSparkのGUI画面を確認することができます\n",
    "# spark.eventLog.enabled true　とすると　GUIで実行ログを確認することができます\n",
    "# GUIなどの確認は次のチャプターで説明を行います。\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"chapter3\") \\\n",
    "    .config(\"hive.exec.dynamic.partition\", \"true\") \\\n",
    "    .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"JST\") \\\n",
    "    .config(\"spark.ui.enabled\",\"true\") \\\n",
    "    .config(\"spark.eventLog.enabled\",\"true\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テーブルを作成しておきます(テーブル名はちょっとだけ変えてjinko_avg2としてます)\n",
    "spark.sql(\"\"\" \n",
    "CREATE EXTERNAL TABLE2 IF NOT EXISTS default.jinko_avg2 ( male_avg double, female_avg double)\n",
    "PARTITIONED BY (kenmei String)\n",
    "STORED AS PARQUET\n",
    "LOCATION '/Users/yuki/pyspark_batch/dataset/parquet/';\n",
    "\"\"\")\n",
    "spark.sql(\" show tables\").show()"
   ]
  },
  {
   "source": [
    "# spark_etl.pyを作成していきます\n",
    "# ここからはノートブックではなくpythonエディターへ移ります。\n",
    "\n",
    "```\n",
    "spark-submit ./spark_etl/spark_etl_sample.py -a hoge -s ./spark_etl/etl.sql -b GENGO=昭和 -t jinko -z ./spark_etl/jinko_schema.json -f /Users/yuki/pyspark_batch/dataset/jinko.csv\n",
    "```\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "実際にクラウド上の実行環境でspark-submitを行う場合は、比較的実行の設定値がラップされていることが多くあまり実行において気にすることはあまりない  \n",
    "オンプレの場合は、細かな設定を行わなければならず苦労するポイントかも知れません。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# チューニングTips\n",
    "\n",
    "Sparkというと、メモリに気をとられドライバーのメモリーやエグゼキューターのメモリについての検索結果がたくさん出てきます。  \n",
    "しかしながら、これらのメモリー設定がどうしても必要になるパターンはまれです(昔はよく設定していた)。\n",
    "\n",
    "メモリー設定を弄りまわす前に確認したいいくつかのポイントを紹介します\n",
    "\n",
    "1. Spark web インタフェースでのボトルネックの確認\n",
    "2. メモリへの登録\n",
    "3. repartition数を大きくしてみる\n",
    "4. Executor(ノード)の台数を増やす(クラウドであれば容易)　メモリ設定などですごく迷うのであれば、さっとノード追加して動かしてしまったほうが結局安上がり\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Spark web インタフェースでのボトルネックの確認\n",
    "Sparkのウェブインタフェースを眺めてみましょう\n",
    "\n",
    "1. sparksessionを起動する\n",
    "2. http://localhost:4040 へアクセスします\n",
    "\n",
    "いくつかの処理を実行して、結果を見比べてみましょう。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\" \n",
    "\n",
    "select * from jinko_avg2\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\" \n",
    "\n",
    "select * from jinko_avg2 where kenmei='三重県'\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\" \n",
    "\n",
    "select * from jinko_avg2 where male_avg > 3000000    \n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "source": [
    "# メモリへの登録\n",
    "処理途中のデータはメモリへ登録することが可能です。  \n",
    "何度も使い回す場合で比較的小さいデータはメモリ登録することで速度の向上が見込めます。\n",
    "\n",
    "メモリの登録にはいくつか種類があります\n",
    "\n",
    "1. dataframeをキャッシュする\n",
    "2. テンポラリテーブルをキャッシュする\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "21/09/10 11:00:52 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n21/09/10 11:00:52 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n21/09/10 11:00:52 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n21/09/10 11:00:52 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# dataframeをキャッシュする場合\n",
    "df=spark.sql(\"\"\" \n",
    "\n",
    "select * from jinko_avg2 where kenmei='三重県'\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "# キャッシュする\n",
    "df.cache()\n",
    "# キャッシュ判定\n",
    "df.is_cached\n",
    "\n",
    "# 後続でdfを使うときはキャッシュから利用される"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark利用の停止\n",
    "spark.stop()\n",
    "spark.sparkContext.stop()"
   ]
  }
 ]
}